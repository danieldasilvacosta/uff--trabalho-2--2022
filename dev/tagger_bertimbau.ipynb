{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca39af7-84d8-469f-aaf6-f5764501dbdb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a53f45d-33b3-4a40-b0e2-674577ca0149",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForPreTraining\n",
    "from transformers import AutoModel \n",
    "BERTIMBAU = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "import pyconll"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a25ab-4f50-4d41-bf1b-338350d16121",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d43634-8572-4c98-98db-1b31c7f76d85",
   "metadata": {},
   "source": [
    "Baseado em: https://github.com/soutsios/pos-tagger-bert/blob/master/pos_tagger_bert.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cd2d9de-b8a8-4963-8f92-dd35e09cfb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "UD_PORTUGUESE_TRAIN = './datasets/pt_bosque-ud-train.conllu'\n",
    "UD_PORTUGUESE_DEV = './datasets/pt_bosque-ud-dev.conllu'\n",
    "UD_PORTUGUESE_TEST = './datasets/pt_bosque-ud-test.conllu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa561387-426b-4c3b-8ddb-6ab686aaae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conllu(path):\n",
    "    data = pyconll.load_from_file(path)\n",
    "    tagged_sentences=[]\n",
    "    t=0\n",
    "    for sentence in data:\n",
    "        tagged_sentence=[]\n",
    "        for token in sentence:\n",
    "            if token.upos and token.form:\n",
    "                t+=1\n",
    "                tagged_sentence.append((token.form.lower(), token.upos))\n",
    "        tagged_sentences.append(tagged_sentence)\n",
    "    return tagged_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da062e3f-04f0-48e3-82db-3c12820115cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = read_conllu(UD_PORTUGUESE_TRAIN)\n",
    "val_sentences = read_conllu(UD_PORTUGUESE_DEV)\n",
    "test_sentences = read_conllu(UD_PORTUGUESE_TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42190221-0f95-4d86-833e-4b509a2a2d68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagged sentences in train set:  7018\n",
      "Tagged words in train set: 171776\n",
      "========================================\n",
      "Tagged sentences in dev set:  1172\n",
      "Tagged words in dev set: 28447\n",
      "========================================\n",
      "Tagged sentences in test set:  1167\n",
      "Tagged words in test set: 27605\n",
      "****************************************\n",
      "Total sentences in dataset: 9357\n"
     ]
    }
   ],
   "source": [
    "print(\"Tagged sentences in train set: \", len(train_sentences))\n",
    "print(\"Tagged words in train set:\", len([item for sublist in train_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in dev set: \", len(val_sentences))\n",
    "print(\"Tagged words in dev set:\", len([item for sublist in val_sentences for item in sublist]))\n",
    "print(40*'=')\n",
    "print(\"Tagged sentences in test set: \", len(test_sentences))\n",
    "print(\"Tagged words in test set:\", len([item for sublist in test_sentences for item in sublist]))\n",
    "print(40*'*')\n",
    "print(\"Total sentences in dataset:\", len(train_sentences)+len(val_sentences)+len(test_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f5b6216-db1b-469b-b735-1df6e4415d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('pt', 'PROPN'), ('em', 'ADP'), ('o', 'DET'), ('governo', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "print(train_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4d651fb-e2cb-44ae-b04d-12d26e756b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some usefull functions\n",
    "def tag_sequence(sentences):\n",
    "    return [[t for w, t in sentence] for sentence in sentences]\n",
    "\n",
    "def text_sequence(sentences):\n",
    "    return [[w for w, t in sentence] for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3e15f6-21a1-46c3-bb97-8851d13e4c09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c6d7d3-ae8e-4f20-a48f-5bad964eda73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5261146a-18b6-4b9b-8cd5-25d342a48a06",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95efe053-4236-4b12-8a96-038b7ff438e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85a21d0-5c8c-4484-b081-a089335a3d26",
   "metadata": {},
   "source": [
    "# CoNLLDataset e Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fa2962b2-a45b-4fff-9d40-83e39a6be36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname, max_exs=None):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname, max_exs)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname, max_exs):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        #print(chunks)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        exs = 0\n",
    "        for chunk in chunks:\n",
    "            if not max_exs or exs < max_exs:\n",
    "                annotation = Annotation()\n",
    "                lines = chunk.split('\\n')\n",
    "                # Iterate over all lines in the chunk.\n",
    "                for line in lines:\n",
    "                    # If line is empty ignore it.\n",
    "                    if len(line)==0:\n",
    "                        continue\n",
    "                    # If line is a commend ignore it.\n",
    "                    if line[0] == '#':\n",
    "                        continue\n",
    "                    # Otherwise split on tabs and retrieve the token and the\n",
    "                    # POS tag fields.\n",
    "                    fields = line.split('\\t')\n",
    "                    annotation.tokens.append(fields[1])\n",
    "                    annotation.pos_tags.append(fields[3])\n",
    "                if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                    annotations.append(annotation)\n",
    "            exs += 1\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4efcd716-40d8-403d-aca2-9bb3ff1cf217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets.\n",
    "train_dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu', 4096)\n",
    "dev_dataset = CoNLLDataset('./datasets/pt_bosque-ud-dev.conllu', 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16110759-f256-41d2-8262-92ba2706e10c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([235, 19, 7, 5, 75], [6, 7, 3, 2, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfe1b3-7bf9-4327-b7c5-704fd5c441b3",
   "metadata": {},
   "source": [
    "# Funções: pad() e collate_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "60721a3f-b03b-452f-83db-3b6cdc5b5a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911a6f5f-aa5d-4201-8b3a-530e8b3265af",
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b37983-1d0e-4cf2-93db-dc60dc75ba0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86030918-8cdc-40d3-b48f-58de4e5eea0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43ee3af-5289-4fe8-ae66-35e57f954654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9513442b-4393-4fb9-b5a2-e9a12dee9430",
   "metadata": {},
   "source": [
    "# Baseado em: https://pytorch.org/hub/huggingface_pytorch-transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d15cd665-3634-492e-9484-cf7825114e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████| 43.0/43.0 [00:00<00:00, 9.49kB/s]\n",
      "Downloading: 100%|█████████| 205k/205k [00:00<00:00, 577kB/s]\n",
      "Downloading: 100%|██████████| 2.00/2.00 [00:00<00:00, 762B/s]\n",
      "Downloading: 100%|██████████| 112/112 [00:00<00:00, 33.2kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)\n",
    "\n",
    "text_1 = \"Who was Jim Henson ?\"\n",
    "text_2 = \"Jim Henson was a puppeteer\"\n",
    "\n",
    "# Tokenized input with special tokens around it (for BERT: [CLS] at the beginning and [SEP] at the end)\n",
    "indexed_tokens = tokenizer.encode(text_1, text_2, add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "35625091-1ec9-4051-882f-ed63b4d702d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForPreTraining were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "segments_tensors = torch.tensor([segments_ids])\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_layers, _ = model(tokens_tensor, token_type_ids=segments_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb304e69-677d-4ddb-8ce3-4361b54d856b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53b6753-134b-41b0-9265-c75d08e4af22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ab88b8-eaa0-4b8d-a78b-e925fda672f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ee45b1-dcf2-4ef4-8e1a-f3c24c7716ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c69aa9c5-ef15-4e6e-87d3-b363ea223978",
   "metadata": {},
   "source": [
    "# Token classification (HuggingFace)\n",
    "https://huggingface.co/docs/transformers/tasks/token_classification\n",
    "https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb#scrollTo=MOsHUjgdIrIW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e4171f-b422-47a2-a09a-06cf1b95498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"ner\" # Should be one of \"ner\", \"pos\" or \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de63afc-f0bb-44af-aeff-68b68e26d8c7",
   "metadata": {},
   "source": [
    "## Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37bc12ce-1cb8-4997-8ff4-1c9ddd0d69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5781e0d6-67ef-44c5-9916-4d780515919d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 9.52kB [00:00, 2.98MB/s]         \n",
      "Downloading metadata: 3.79kB [00:00, 549kB/s]                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /home/danieldasilvacosta/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|████| 983k/983k [00:01<00:00, 903kB/s]\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /home/danieldasilvacosta/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████| 3/3 [00:00<00:00, 274.26it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afbafd06-1ad7-45e5-a3ac-672b90374e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 14042\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3251\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
       "        num_rows: 3454\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbace6ea-4d81-4128-8871-66d9e3855b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16bb5122-3b85-4853-ac34-3761cc35d8ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=ClassLabel(num_classes=9, names=['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'], id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"].features[f\"ner_tags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6f3d0b0-06b0-48c6-b375-293cd18803bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805d1d69-4463-44b0-911e-9a1c498cbf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "542993c8-fc75-4a4f-9aec-dfade63724d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos_tags</th>\n",
       "      <th>chunk_tags</th>\n",
       "      <th>ner_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8565</td>\n",
       "      <td>[FLORIDA, AT, ST, LOUIS]</td>\n",
       "      <td>[NNP, NNP, NNP, NNP]</td>\n",
       "      <td>[B-NP, I-NP, I-NP, I-NP]</td>\n",
       "      <td>[B-ORG, O, B-LOC, I-LOC]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>546</td>\n",
       "      <td>[Attendance, :, 3,000]</td>\n",
       "      <td>[NN, :, CD]</td>\n",
       "      <td>[B-NP, O, B-NP]</td>\n",
       "      <td>[O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7043</td>\n",
       "      <td>[Alex, O'Brien, (, U.S., ), beat, Nicolas, Lapentti, (, Ecuador, ), 6-4, 1-6, 6-4, 6-3]</td>\n",
       "      <td>[NNP, NNP, (, NNP, ), VB, NNP, NNP, (, NNP, ), CD, CD, CD, CD]</td>\n",
       "      <td>[B-NP, I-NP, O, B-NP, O, B-VP, B-NP, I-NP, O, B-NP, O, B-NP, I-NP, I-NP, I-NP]</td>\n",
       "      <td>[B-PER, I-PER, O, B-LOC, O, O, B-PER, I-PER, O, B-LOC, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10614</td>\n",
       "      <td>[SOCCER, -, GERMAN, FIRST, DIVISION, SUMMARIES, .]</td>\n",
       "      <td>[NN, :, NNP, NNP, NNP, NNP, .]</td>\n",
       "      <td>[B-NP, O, B-NP, I-NP, I-NP, I-NP, O]</td>\n",
       "      <td>[O, O, B-MISC, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1821</td>\n",
       "      <td>[Kiev, 's, Foreign, Minister, Hennady, Udovenko, said, Beijing, was, overreacting, .]</td>\n",
       "      <td>[NNP, POS, NNP, NNP, NNP, NNP, VBD, NNP, VBD, VBG, .]</td>\n",
       "      <td>[B-NP, B-NP, I-NP, I-NP, I-NP, I-NP, B-VP, B-NP, B-VP, I-VP, O]</td>\n",
       "      <td>[B-LOC, O, O, O, B-PER, I-PER, O, B-LOC, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11663</td>\n",
       "      <td>[The, unexpectedly, weak, figures, convinced, markets, that, Japanese, interest, rates, would, stay, at, rock-bottom, levels, --, weakening, the, yen, .]</td>\n",
       "      <td>[DT, RB, JJ, NNS, VBN, NNS, IN, JJ, NN, NNS, MD, VB, IN, JJ, NNS, :, VBG, DT, NN, .]</td>\n",
       "      <td>[B-NP, I-NP, I-NP, I-NP, B-VP, B-NP, B-SBAR, B-NP, I-NP, I-NP, B-VP, I-VP, B-PP, B-NP, I-NP, O, B-VP, B-NP, I-NP, O]</td>\n",
       "      <td>[O, O, O, O, O, O, O, B-MISC, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>13920</td>\n",
       "      <td>[Rubin, was, widely, hailed, as, the, architect, of, the, dollar, 's, comeback, ,, using, skills, and, expertise, gained, in, 26, years, on, Wall, Street, ,, part, of, which, were, spent, as, co-chairman, of, Goldman, ,, Sachs, and, Co, .]</td>\n",
       "      <td>[NNP, VBD, RB, VBN, IN, DT, NN, IN, DT, NN, POS, NN, ,, VBG, NNS, CC, NN, VBD, IN, CD, NNS, IN, NNP, NNP, ,, NN, IN, WDT, VBD, VBN, IN, NN, IN, NNP, ,, NNP, CC, NNP, .]</td>\n",
       "      <td>[B-NP, B-VP, I-VP, I-VP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, B-NP, I-NP, O, B-VP, B-NP, I-NP, I-NP, B-VP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, O, B-NP, B-PP, B-NP, O, B-VP, B-PP, B-NP, B-PP, B-NP, I-NP, I-NP, I-NP, I-NP, O]</td>\n",
       "      <td>[B-PER, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-LOC, I-LOC, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, I-ORG, I-ORG, I-ORG]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12918</td>\n",
       "      <td>[Congress, is, constitutionally, obliged, to, approve, the, budget, by, the, end, of, year, but, regularly, fails, to, meet, that, requirement, .]</td>\n",
       "      <td>[NNP, VBZ, RB, JJ, TO, VB, DT, NN, IN, DT, NN, IN, NN, CC, RB, VBZ, TO, VB, DT, NN, .]</td>\n",
       "      <td>[B-NP, B-VP, B-NP, B-ADJP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O, B-VP, I-VP, I-VP, I-VP, B-NP, I-NP, O]</td>\n",
       "      <td>[B-ORG, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>11195</td>\n",
       "      <td>[Bulgaria, has, restored, ownership, rights, to, pre-communist, private, owners, of, 75, percent, of, the, arable, land, or, around, four, million, hectares, ,, an, Agriculture, Ministry, official, said, on, Wednesday, .]</td>\n",
       "      <td>[NNP, VBZ, VBN, NN, NNS, TO, VB, JJ, NNS, IN, CD, NN, IN, DT, JJ, NN, CC, IN, CD, CD, NNS, ,, DT, NNP, NNP, NN, VBD, IN, NNP, .]</td>\n",
       "      <td>[B-NP, B-VP, I-VP, B-NP, I-NP, B-VP, I-VP, B-NP, I-NP, B-PP, B-NP, I-NP, B-PP, B-NP, I-NP, I-NP, I-NP, B-PP, B-NP, I-NP, I-NP, O, B-NP, I-NP, I-NP, I-NP, B-VP, B-PP, B-NP, O]</td>\n",
       "      <td>[B-LOC, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, B-ORG, I-ORG, O, O, O, O, O]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8971</td>\n",
       "      <td>[During, his, visit, to, Slovenia, ,, Kwasniewski, is, also, scheduled, to, meet, Prime, Minister, Janez, Drnovsek, ,, representatives, of, Slovenian, political, parties, and, representatives, of, the, Chamber, of, Economy, .]</td>\n",
       "      <td>[IN, PRP$, NN, TO, NNP, ,, NNP, VBZ, RB, VBN, TO, VB, NNP, NNP, NNP, NNP, ,, NNS, IN, JJ, JJ, NNS, CC, NNS, IN, DT, NNP, IN, NNP, .]</td>\n",
       "      <td>[B-PP, B-NP, I-NP, B-PP, B-NP, O, B-NP, B-VP, I-VP, I-VP, I-VP, I-VP, B-NP, I-NP, I-NP, I-NP, O, B-NP, B-PP, B-NP, I-NP, I-NP, O, B-NP, B-PP, B-NP, I-NP, B-PP, B-NP, O]</td>\n",
       "      <td>[O, O, O, O, B-LOC, O, B-PER, O, O, O, O, O, O, O, B-PER, I-PER, O, O, O, B-MISC, O, O, O, O, O, O, B-ORG, I-ORG, I-ORG, O]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222bdd09-4660-40dc-9ab8-9fe7b3827a0e",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b05d5879-b04e-41b4-a020-3e521e1c0bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████| 28.0/28.0 [00:00<00:00, 17.9kB/s]\n",
      "Downloading: 100%|███████████| 483/483 [00:00<00:00, 180kB/s]\n",
      "Downloading: 100%|█████████| 226k/226k [00:00<00:00, 484kB/s]\n",
      "Downloading: 100%|█████████| 455k/455k [00:00<00:00, 640kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "feb66db8-1582-43cf-8ad2-71f585b2d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1f2112d5-deb6-4407-ad82-64cdb129fac1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 999, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(\"Hello, this is one sentence!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b285a4a-69cf-4f70-80d2-41c79ea28833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 7592, 1010, 2023, 2003, 2028, 6251, 3975, 2046, 2616, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer([\"Hello\", \",\", \"this\", \"is\", \"one\", \"sentence\", \"split\", \"into\", \"words\", \".\"], is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8446a551-2d3c-4a5b-83a1-91a552a99295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Germany', \"'s\", 'representative', 'to', 'the', 'European', 'Union', \"'s\", 'veterinary', 'committee', 'Werner', 'Zwingmann', 'said', 'on', 'Wednesday', 'consumers', 'should', 'buy', 'sheepmeat', 'from', 'countries', 'other', 'than', 'Britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.']\n"
     ]
    }
   ],
   "source": [
    "example = datasets[\"train\"][4]\n",
    "print(example[\"tokens\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4fb7d0b5-caf8-46d6-adb8-8c59f3ed7d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'germany', \"'\", 's', 'representative', 'to', 'the', 'european', 'union', \"'\", 's', 'veterinary', 'committee', 'werner', 'z', '##wing', '##mann', 'said', 'on', 'wednesday', 'consumers', 'should', 'buy', 'sheep', '##me', '##at', 'from', 'countries', 'other', 'than', 'britain', 'until', 'the', 'scientific', 'advice', 'was', 'clearer', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokenized_input = tokenizer(example[\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "74b38be7-f368-4dbb-9b09-2b73a9816905",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 39)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(example[f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "821afbcf-86d9-4967-81fe-c1a0d1b24701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 11, 11, 11, 12, 13, 14, 15, 16, 17, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, None]\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_input.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dbab9fee-ae3f-446e-8588-f160373aa014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 39\n"
     ]
    }
   ],
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else example[f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0eeb9fdd-5827-4ef4-becb-5d27c105aee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_all_tokens = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8971515c-3b12-40de-b1e0-c3cf45f3f7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]): # class (pos_tags ou chunk_tags ou ner_tags)\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
    "            # ignored in the loss function.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # We set the label for the first token of each word.\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
    "            # the label_all_tokens flag.\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d12594bc-3000-4423-96cb-5ae47e24f3e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 7327, 19164, 2446, 2655, 2000, 17757, 2329, 12559, 1012, 102], [101, 2848, 13934, 102], [101, 9371, 2727, 1011, 5511, 1011, 2570, 102], [101, 1996, 2647, 3222, 2056, 2006, 9432, 2009, 18335, 2007, 2446, 6040, 2000, 10390, 2000, 18454, 2078, 2329, 12559, 2127, 6529, 5646, 3251, 5506, 11190, 4295, 2064, 2022, 11860, 2000, 8351, 1012, 102], [101, 2762, 1005, 1055, 4387, 2000, 1996, 2647, 2586, 1005, 1055, 15651, 2837, 14121, 1062, 9328, 5804, 2056, 2006, 9317, 10390, 2323, 4965, 8351, 4168, 4017, 2013, 3032, 2060, 2084, 3725, 2127, 1996, 4045, 6040, 2001, 24509, 1012, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100], [-100, 1, 2, -100], [-100, 5, 0, 0, 0, 0, 0, -100], [-100, 0, 3, 4, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100], [-100, 5, 0, 0, 0, 0, 0, 3, 4, 0, 0, 0, 0, 1, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, -100]]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_and_align_labels(datasets['train'][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ff1709b2-4b81-473d-a9c2-c6af445d5064",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/danieldasilvacosta/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-bd9cc90169c59e2a.arrow\n",
      "Loading cached processed dataset at /home/danieldasilvacosta/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-23e607d2d5ae4965.arrow\n",
      "Loading cached processed dataset at /home/danieldasilvacosta/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee/cache-115bf85215420fa9.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ea470d60-17d4-4170-9cfa-85980735454e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a63e2271-8e6d-408d-9672-8eb33b347043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0],\n",
       " 'input_ids': [101,\n",
       "  7327,\n",
       "  19164,\n",
       "  2446,\n",
       "  2655,\n",
       "  2000,\n",
       "  17757,\n",
       "  2329,\n",
       "  12559,\n",
       "  1012,\n",
       "  102],\n",
       " 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " 'labels': [-100, 3, 0, 7, 0, 0, 0, 7, 0, 0, -100]}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d2c544-cc96-4209-a088-1c59ef20ea84",
   "metadata": {},
   "source": [
    "## Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0fa89144-26fc-422f-ae53-2b1661f1a90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|████████| 256M/256M [00:12<00:00, 21.2MB/s]\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForTokenClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "06eb9252-9ce5-4b4d-afb5-5a5a1f9b5de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-{task}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "192c0dd8-e723-43e1-86db-aa3c61c63acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6a350942-60a9-4d33-b711-61819a6d65d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.33kB [00:00, 2.42MB/s]         \n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "35ec4352-e905-4baa-a124-87271e9df097",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LOC': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 2},\n",
       " 'ORG': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'PER': {'precision': 1.0, 'recall': 1.0, 'f1': 1.0, 'number': 1},\n",
       " 'overall_precision': 1.0,\n",
       " 'overall_recall': 1.0,\n",
       " 'overall_f1': 1.0,\n",
       " 'overall_accuracy': 1.0}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = [label_list[i] for i in example[f\"{task}_tags\"]]\n",
    "metric.compute(predictions=[labels], references=[labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "92380166-9297-4140-a92a-6c2270fa9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eea65a4-54c8-4b6d-9898-ebf5c96b5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "83b1c3ff-6c02-4412-81ef-322483096f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `DistilBertForTokenClassification.forward` and have been ignored: tokens, id, chunk_tags, ner_tags, pos_tags. If tokens, id, chunk_tags, ner_tags, pos_tags are not expected by `DistilBertForTokenClassification.forward`,  you can safely ignore this message.\n",
      "/home/danieldasilvacosta/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14042\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 2634\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='2634' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  52/2634 01:30 < 1:18:07, 0.55 it/s, Epoch 0.06/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [61]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/transformers/trainer.py:1317\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1314\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1315\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1316\u001b[0m )\n\u001b[0;32m-> 1317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1318\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/transformers/trainer.py:1619\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     optimizer_was_run \u001b[38;5;241m=\u001b[39m scale_before \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m scale_after\n\u001b[1;32m   1618\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1619\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1621\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optimizer_was_run \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[1;32m   1622\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:65\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     64\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev/doutorado/uff--trabalho-2--2022/lib/python3.8/site-packages/transformers/optimization.py:361\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    359\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mmul_(beta1)\u001b[38;5;241m.\u001b[39madd_(grad, alpha\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1))\n\u001b[1;32m    360\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m--> 361\u001b[0m denom \u001b[38;5;241m=\u001b[39m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39madd_(group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    363\u001b[0m step_size \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_bias\u001b[39m\u001b[38;5;124m\"\u001b[39m]:  \u001b[38;5;66;03m# No bias correction for Bert\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7469babc-4a3e-4c13-a3a5-96b483a0c2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6529756-e917-454c-9f24-6e7345f64e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b70de2-f50b-47b9-aea8-380d3c86a9b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e09ab3-9be3-4991-b782-21659a24fc97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20927c77-fd51-4a68-b471-8b7d67077785",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
