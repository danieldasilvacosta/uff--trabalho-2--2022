{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca39af7-84d8-469f-aaf6-f5764501dbdb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a53f45d-33b3-4a40-b0e2-674577ca0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc157566-3db0-4482-92bb-726d624bc86e",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f6eae-e285-47f4-b463-172ebc32624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv( \n",
    "#     './datasets/pt_bosque-ud-train--pos.conllu',\n",
    "#     sep = '\\t' )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa1169a-652f-40d3-a24a-4ab70cf040d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train = df[ df.iloc[:,3] != '_' ]\n",
    "# df_pos_train = df_pos_train[ [ '_.1', '_.3' ] ]\n",
    "# df_pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bc03a4-4e6f-466e-b704-756cc2a76718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd289cb-2489-45b5-84de-0b50f9a8e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42277ed-49e8-4793-8a0a-b8019fb079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_train.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa12d98-6f6d-4c09-ad93-8f373ac3acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af9c142-4e59-458a-9d67-dffa944f7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_train.iloc[:, 0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3ed17-26ba-4b8b-9de9-308603cf9468",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcfd7969-20c1-41e1-8ed1-a8f045acfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv( \n",
    "#     './datasets/pt_bosque-ud-dev--pos.conllu',\n",
    "#     sep = '\\t' )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64eb7acd-da9f-4203-9b4e-1a56cc2a2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev = df[ df.iloc[:,3] != '_' ]\n",
    "# df_pos_dev = df_pos_dev[ [ '_.1', '_.3' ] ]\n",
    "# df_pos_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c600b44-4e82-41cd-8c6e-15c117757340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddb54a2-c835-48b8-8631-b9535b9af432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9a82cb-0c19-42c4-852c-692b0229607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_dev.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03645fb0-1042-414e-af3e-4f8a3f5976ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6793d8b2-3aa4-457d-ab28-b8cc035cb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_dev.iloc[:, 0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beefaac-3b23-4d8a-b2d1-a337eb0e8c33",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8dbcfa-a7fe-4393-95c6-ee59e7c0da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94ad8b-ff9e-4d97-be37-2fad7f172072",
   "metadata": {},
   "source": [
    "# CoNLLDataset e Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef8512e-d6b2-4e96-a9c8-7fcb464af35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname, max_exs=None):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname, max_exs)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname, max_exs):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        #print(chunks)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        exs = 0\n",
    "        for chunk in chunks:\n",
    "            if not max_exs or exs < max_exs:\n",
    "                annotation = Annotation()\n",
    "                lines = chunk.split('\\n')\n",
    "                # Iterate over all lines in the chunk.\n",
    "                for line in lines:\n",
    "                    # If line is empty ignore it.\n",
    "                    if len(line)==0:\n",
    "                        continue\n",
    "                    # If line is a commend ignore it.\n",
    "                    if line[0] == '#':\n",
    "                        continue\n",
    "                    # Otherwise split on tabs and retrieve the token and the\n",
    "                    # POS tag fields.\n",
    "                    fields = line.split('\\t')\n",
    "                    annotation.tokens.append(fields[1])\n",
    "                    annotation.pos_tags.append(fields[3])\n",
    "                if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                    annotations.append(annotation)\n",
    "            exs += 1\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5907be-6c34-4e65-b922-9e814b771922",
   "metadata": {},
   "source": [
    "# Funções: pad() e collate_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22bfb3a3-52e1-4c2c-8563-723e15182391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecfc14-2652-47be-895b-3a89b6e65db9",
   "metadata": {},
   "source": [
    "# Tagger - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09b9f91-6d8c-40b7-b21b-e8838c66e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 embedding_dim=64,\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True,\n",
    "                 pad_idx=0):\n",
    "        \"\"\"Initializes the tagger.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Size of the input vocabulary, projection\n",
    "            output_dim: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_dim: Number of units in each LSTM hidden layer.\n",
    "            bidirectional: Whether or not to use a bidirectional rnn.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_dim = input_dim \n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "          \n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "        \n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "            \n",
    "        Returns:\n",
    "            net: Probability of the next word in the sequence.\n",
    "            hidden: Hidden state of the lstm.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "        \n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            if self.bidirectional:\n",
    "                num_directions = 2\n",
    "            else:\n",
    "                num_directions = 1\n",
    "            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_dim))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        # Pack before feeding into the RNN.\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths)\n",
    "        # net, hidden = self.rnn(net, hidden) # Daniel\n",
    "        net, hidden = self.rnn(net, (hidden, hidden)) # Daniel\n",
    "        # Unpack after\n",
    "        if lengths is not None:\n",
    "            net, _ = pad_packed_sequence(net)\n",
    "        net = self.fc(net)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0c518-c0a6-42bb-bf88-c37e55c0fbd9",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020c10db-51d4-4bec-8ba7-829d3bf554af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0 - Train Loss: 2.949664 - Dev Loss: 2.923160\n",
      "Epoch 0 Iteration 10 - Train Loss: 2.837316 - Dev Loss: 2.704585\n",
      "Epoch 0 Iteration 20 - Train Loss: 2.585007 - Dev Loss: 2.422829\n",
      "Epoch 0 Iteration 30 - Train Loss: 2.279603 - Dev Loss: 2.093273\n",
      "Epoch 0 Iteration 40 - Train Loss: 2.015233 - Dev Loss: 1.884089\n",
      "Epoch 0 Iteration 50 - Train Loss: 1.761108 - Dev Loss: 1.710888\n",
      "Epoch 0 Iteration 60 - Train Loss: 1.624144 - Dev Loss: 1.564820\n",
      "Epoch 0 Iteration 70 - Train Loss: 1.518446 - Dev Loss: 1.440840\n",
      "Epoch 0 Iteration 80 - Train Loss: 1.401041 - Dev Loss: 1.335876\n",
      "Epoch 0 Iteration 90 - Train Loss: 1.327154 - Dev Loss: 1.248139\n",
      "Epoch 0 Iteration 100 - Train Loss: 1.189110 - Dev Loss: 1.180796\n",
      "Epoch 0 Iteration 110 - Train Loss: 1.113048 - Dev Loss: 1.133769\n",
      "Epoch 0 Iteration 120 - Train Loss: 1.174519 - Dev Loss: 1.081673\n",
      "Epoch 0 Iteration 130 - Train Loss: 1.118218 - Dev Loss: 1.042952\n",
      "Epoch 0 Iteration 140 - Train Loss: 1.059166 - Dev Loss: 1.009109\n",
      "Epoch 0 Iteration 150 - Train Loss: 1.006564 - Dev Loss: 0.980346\n",
      "Epoch 0 Iteration 160 - Train Loss: 0.991098 - Dev Loss: 0.955139\n",
      "Epoch 0 Iteration 170 - Train Loss: 1.000905 - Dev Loss: 0.931317\n",
      "Epoch 0 Iteration 180 - Train Loss: 0.956968 - Dev Loss: 0.916959\n",
      "Epoch 0 Iteration 190 - Train Loss: 0.955354 - Dev Loss: 0.891091\n",
      "Epoch 0 Iteration 200 - Train Loss: 0.954481 - Dev Loss: 0.873243\n",
      "Epoch 0 Iteration 210 - Train Loss: 0.927535 - Dev Loss: 0.858082\n",
      "Epoch 0 Iteration 220 - Train Loss: 0.892734 - Dev Loss: 0.847921\n",
      "Epoch 0 Iteration 230 - Train Loss: 0.869222 - Dev Loss: 0.831768\n",
      "Epoch 0 Iteration 240 - Train Loss: 0.888241 - Dev Loss: 0.819254\n",
      "Epoch 0 Iteration 250 - Train Loss: 0.867498 - Dev Loss: 0.806520\n",
      "Epoch 1 Iteration 260 - Train Loss: 0.829231 - Dev Loss: 0.793753\n",
      "Epoch 1 Iteration 270 - Train Loss: 0.831446 - Dev Loss: 0.785644\n",
      "Epoch 1 Iteration 280 - Train Loss: 0.764093 - Dev Loss: 0.772408\n",
      "Epoch 1 Iteration 290 - Train Loss: 0.784120 - Dev Loss: 0.761704\n",
      "Epoch 1 Iteration 300 - Train Loss: 0.749855 - Dev Loss: 0.753945\n",
      "Epoch 1 Iteration 310 - Train Loss: 0.749773 - Dev Loss: 0.743514\n",
      "Epoch 1 Iteration 320 - Train Loss: 0.735916 - Dev Loss: 0.734117\n",
      "Epoch 1 Iteration 330 - Train Loss: 0.752137 - Dev Loss: 0.729819\n",
      "Epoch 1 Iteration 340 - Train Loss: 0.708273 - Dev Loss: 0.717208\n",
      "Epoch 1 Iteration 350 - Train Loss: 0.716833 - Dev Loss: 0.708591\n",
      "Epoch 1 Iteration 360 - Train Loss: 0.744950 - Dev Loss: 0.701186\n",
      "Epoch 1 Iteration 370 - Train Loss: 0.723336 - Dev Loss: 0.693804\n",
      "Epoch 1 Iteration 380 - Train Loss: 0.670333 - Dev Loss: 0.691362\n",
      "Epoch 1 Iteration 390 - Train Loss: 0.716709 - Dev Loss: 0.677437\n",
      "Epoch 1 Iteration 400 - Train Loss: 0.694537 - Dev Loss: 0.671133\n",
      "Epoch 1 Iteration 410 - Train Loss: 0.697738 - Dev Loss: 0.664856\n",
      "Epoch 1 Iteration 420 - Train Loss: 0.721830 - Dev Loss: 0.657246\n",
      "Epoch 1 Iteration 430 - Train Loss: 0.686584 - Dev Loss: 0.654579\n",
      "Epoch 1 Iteration 440 - Train Loss: 0.690806 - Dev Loss: 0.648297\n",
      "Epoch 1 Iteration 450 - Train Loss: 0.659978 - Dev Loss: 0.641713\n",
      "Epoch 1 Iteration 460 - Train Loss: 0.663604 - Dev Loss: 0.634174\n",
      "Epoch 1 Iteration 470 - Train Loss: 0.588327 - Dev Loss: 0.626797\n",
      "Epoch 1 Iteration 480 - Train Loss: 0.650951 - Dev Loss: 0.625576\n",
      "Epoch 1 Iteration 490 - Train Loss: 0.630413 - Dev Loss: 0.617751\n",
      "Epoch 1 Iteration 500 - Train Loss: 0.640968 - Dev Loss: 0.614392\n",
      "Epoch 1 Iteration 510 - Train Loss: 0.629522 - Dev Loss: 0.608628\n",
      "Epoch 2 Iteration 520 - Train Loss: 0.627327 - Dev Loss: 0.609678\n",
      "Epoch 2 Iteration 530 - Train Loss: 0.568545 - Dev Loss: 0.601366\n",
      "Epoch 2 Iteration 540 - Train Loss: 0.571148 - Dev Loss: 0.591736\n",
      "Epoch 2 Iteration 550 - Train Loss: 0.565439 - Dev Loss: 0.588301\n",
      "Epoch 2 Iteration 560 - Train Loss: 0.592075 - Dev Loss: 0.583206\n",
      "Epoch 2 Iteration 570 - Train Loss: 0.591898 - Dev Loss: 0.581411\n",
      "Epoch 2 Iteration 580 - Train Loss: 0.547978 - Dev Loss: 0.579813\n",
      "Epoch 2 Iteration 590 - Train Loss: 0.551260 - Dev Loss: 0.572077\n",
      "Epoch 2 Iteration 600 - Train Loss: 0.588075 - Dev Loss: 0.566755\n",
      "Epoch 2 Iteration 610 - Train Loss: 0.509844 - Dev Loss: 0.564470\n",
      "Epoch 2 Iteration 620 - Train Loss: 0.602972 - Dev Loss: 0.557493\n",
      "Epoch 2 Iteration 630 - Train Loss: 0.548559 - Dev Loss: 0.557581\n",
      "Epoch 2 Iteration 640 - Train Loss: 0.534466 - Dev Loss: 0.550016\n",
      "Epoch 2 Iteration 650 - Train Loss: 0.548443 - Dev Loss: 0.549530\n",
      "Epoch 2 Iteration 660 - Train Loss: 0.485451 - Dev Loss: 0.546672\n",
      "Epoch 2 Iteration 670 - Train Loss: 0.514314 - Dev Loss: 0.543480\n",
      "Epoch 2 Iteration 680 - Train Loss: 0.502583 - Dev Loss: 0.541008\n",
      "Epoch 2 Iteration 690 - Train Loss: 0.534175 - Dev Loss: 0.536148\n",
      "Epoch 2 Iteration 700 - Train Loss: 0.529568 - Dev Loss: 0.534867\n",
      "Epoch 2 Iteration 710 - Train Loss: 0.543371 - Dev Loss: 0.527884\n",
      "Epoch 2 Iteration 720 - Train Loss: 0.511297 - Dev Loss: 0.524288\n",
      "Epoch 2 Iteration 730 - Train Loss: 0.514820 - Dev Loss: 0.522256\n",
      "Epoch 2 Iteration 740 - Train Loss: 0.507836 - Dev Loss: 0.518532\n",
      "Epoch 2 Iteration 750 - Train Loss: 0.507400 - Dev Loss: 0.529022\n",
      "Epoch 2 Iteration 760 - Train Loss: 0.483476 - Dev Loss: 0.512598\n",
      "Epoch 3 Iteration 770 - Train Loss: 0.511587 - Dev Loss: 0.510920\n",
      "Epoch 3 Iteration 780 - Train Loss: 0.475155 - Dev Loss: 0.515452\n",
      "Epoch 3 Iteration 790 - Train Loss: 0.438247 - Dev Loss: 0.509087\n",
      "Epoch 3 Iteration 800 - Train Loss: 0.434190 - Dev Loss: 0.508034\n",
      "Epoch 3 Iteration 810 - Train Loss: 0.493262 - Dev Loss: 0.506283\n",
      "Epoch 3 Iteration 820 - Train Loss: 0.466046 - Dev Loss: 0.499845\n",
      "Epoch 3 Iteration 830 - Train Loss: 0.441371 - Dev Loss: 0.503798\n",
      "Epoch 3 Iteration 840 - Train Loss: 0.455950 - Dev Loss: 0.493497\n",
      "Epoch 3 Iteration 850 - Train Loss: 0.456705 - Dev Loss: 0.490527\n",
      "Epoch 3 Iteration 860 - Train Loss: 0.463279 - Dev Loss: 0.486663\n",
      "Epoch 3 Iteration 870 - Train Loss: 0.423440 - Dev Loss: 0.486126\n",
      "Epoch 3 Iteration 880 - Train Loss: 0.477789 - Dev Loss: 0.487972\n",
      "Epoch 3 Iteration 890 - Train Loss: 0.444304 - Dev Loss: 0.481181\n",
      "Epoch 3 Iteration 900 - Train Loss: 0.431759 - Dev Loss: 0.484251\n",
      "Epoch 3 Iteration 910 - Train Loss: 0.415828 - Dev Loss: 0.477175\n",
      "Epoch 3 Iteration 920 - Train Loss: 0.420995 - Dev Loss: 0.474416\n",
      "Epoch 3 Iteration 930 - Train Loss: 0.428389 - Dev Loss: 0.477837\n",
      "Epoch 3 Iteration 940 - Train Loss: 0.448144 - Dev Loss: 0.473185\n",
      "Epoch 3 Iteration 950 - Train Loss: 0.426734 - Dev Loss: 0.466038\n",
      "Epoch 3 Iteration 960 - Train Loss: 0.412142 - Dev Loss: 0.462572\n",
      "Epoch 3 Iteration 970 - Train Loss: 0.439929 - Dev Loss: 0.459596\n",
      "Epoch 3 Iteration 980 - Train Loss: 0.442588 - Dev Loss: 0.461516\n",
      "Epoch 3 Iteration 990 - Train Loss: 0.408226 - Dev Loss: 0.463173\n",
      "Epoch 3 Iteration 1000 - Train Loss: 0.387074 - Dev Loss: 0.459210\n",
      "Epoch 3 Iteration 1010 - Train Loss: 0.400844 - Dev Loss: 0.453955\n",
      "Epoch 3 Iteration 1020 - Train Loss: 0.420349 - Dev Loss: 0.456469\n",
      "Epoch 4 Iteration 1030 - Train Loss: 0.405202 - Dev Loss: 0.449742\n",
      "Epoch 4 Iteration 1040 - Train Loss: 0.346003 - Dev Loss: 0.452180\n",
      "Epoch 4 Iteration 1050 - Train Loss: 0.382783 - Dev Loss: 0.443972\n",
      "Epoch 4 Iteration 1060 - Train Loss: 0.355220 - Dev Loss: 0.455625\n",
      "Epoch 4 Iteration 1070 - Train Loss: 0.369701 - Dev Loss: 0.441429\n",
      "Epoch 4 Iteration 1080 - Train Loss: 0.380611 - Dev Loss: 0.447822\n",
      "Epoch 4 Iteration 1090 - Train Loss: 0.364274 - Dev Loss: 0.437948\n",
      "Epoch 4 Iteration 1100 - Train Loss: 0.360271 - Dev Loss: 0.439847\n",
      "Epoch 4 Iteration 1110 - Train Loss: 0.369983 - Dev Loss: 0.437557\n",
      "Epoch 4 Iteration 1120 - Train Loss: 0.378692 - Dev Loss: 0.437201\n",
      "Epoch 4 Iteration 1130 - Train Loss: 0.370203 - Dev Loss: 0.444507\n",
      "Epoch 4 Iteration 1140 - Train Loss: 0.371345 - Dev Loss: 0.431677\n",
      "Epoch 4 Iteration 1150 - Train Loss: 0.365996 - Dev Loss: 0.440269\n",
      "Epoch 4 Iteration 1160 - Train Loss: 0.355824 - Dev Loss: 0.437998\n",
      "Epoch 4 Iteration 1170 - Train Loss: 0.339516 - Dev Loss: 0.429780\n",
      "Epoch 4 Iteration 1180 - Train Loss: 0.374788 - Dev Loss: 0.441272\n",
      "Epoch 4 Iteration 1190 - Train Loss: 0.353691 - Dev Loss: 0.426151\n",
      "Epoch 4 Iteration 1200 - Train Loss: 0.358362 - Dev Loss: 0.423735\n",
      "Epoch 4 Iteration 1210 - Train Loss: 0.360718 - Dev Loss: 0.432983\n",
      "Epoch 4 Iteration 1220 - Train Loss: 0.349604 - Dev Loss: 0.419132\n",
      "Epoch 4 Iteration 1230 - Train Loss: 0.334883 - Dev Loss: 0.426330\n",
      "Epoch 4 Iteration 1240 - Train Loss: 0.355140 - Dev Loss: 0.423533\n",
      "Epoch 4 Iteration 1250 - Train Loss: 0.336302 - Dev Loss: 0.414448\n",
      "Epoch 4 Iteration 1260 - Train Loss: 0.327356 - Dev Loss: 0.417282\n",
      "Epoch 4 Iteration 1270 - Train Loss: 0.348105 - Dev Loss: 0.412653\n",
      "Epoch 5 Iteration 1280 - Train Loss: 0.325464 - Dev Loss: 0.412719\n",
      "Epoch 5 Iteration 1290 - Train Loss: 0.287356 - Dev Loss: 0.417657\n",
      "Epoch 5 Iteration 1300 - Train Loss: 0.290384 - Dev Loss: 0.409547\n",
      "Epoch 5 Iteration 1310 - Train Loss: 0.304588 - Dev Loss: 0.407840\n",
      "Epoch 5 Iteration 1320 - Train Loss: 0.321689 - Dev Loss: 0.405583\n",
      "Epoch 5 Iteration 1330 - Train Loss: 0.330064 - Dev Loss: 0.414781\n",
      "Epoch 5 Iteration 1340 - Train Loss: 0.287408 - Dev Loss: 0.407863\n",
      "Epoch 5 Iteration 1350 - Train Loss: 0.281014 - Dev Loss: 0.406859\n",
      "Epoch 5 Iteration 1360 - Train Loss: 0.318943 - Dev Loss: 0.400864\n",
      "Epoch 5 Iteration 1370 - Train Loss: 0.294885 - Dev Loss: 0.407333\n",
      "Epoch 5 Iteration 1380 - Train Loss: 0.348706 - Dev Loss: 0.398114\n",
      "Epoch 5 Iteration 1390 - Train Loss: 0.277547 - Dev Loss: 0.396975\n",
      "Epoch 5 Iteration 1400 - Train Loss: 0.304918 - Dev Loss: 0.402572\n",
      "Epoch 5 Iteration 1410 - Train Loss: 0.327683 - Dev Loss: 0.405449\n",
      "Epoch 5 Iteration 1420 - Train Loss: 0.280853 - Dev Loss: 0.392599\n",
      "Epoch 5 Iteration 1430 - Train Loss: 0.276998 - Dev Loss: 0.394092\n",
      "Epoch 5 Iteration 1440 - Train Loss: 0.272652 - Dev Loss: 0.396138\n",
      "Epoch 5 Iteration 1450 - Train Loss: 0.277959 - Dev Loss: 0.395290\n",
      "Epoch 5 Iteration 1460 - Train Loss: 0.281371 - Dev Loss: 0.390682\n",
      "Epoch 5 Iteration 1470 - Train Loss: 0.282349 - Dev Loss: 0.393690\n",
      "Epoch 5 Iteration 1480 - Train Loss: 0.268045 - Dev Loss: 0.396566\n",
      "Epoch 5 Iteration 1490 - Train Loss: 0.269813 - Dev Loss: 0.408026\n",
      "Epoch 5 Iteration 1500 - Train Loss: 0.289658 - Dev Loss: 0.400374\n",
      "Epoch 5 Iteration 1510 - Train Loss: 0.272208 - Dev Loss: 0.390949\n",
      "Epoch 5 Iteration 1520 - Train Loss: 0.282072 - Dev Loss: 0.395695\n",
      "Epoch 5 Iteration 1530 - Train Loss: 0.293385 - Dev Loss: 0.392616\n"
     ]
    }
   ],
   "source": [
    "# Load datasets.\n",
    "train_dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu', 4096)\n",
    "dev_dataset = CoNLLDataset('./datasets/pt_bosque-ud-dev.conllu', 1024)\n",
    "\n",
    "dev_dataset.token_vocab = train_dataset.token_vocab\n",
    "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(train_dataset.token_vocab)\n",
    "output_vocab_size = len(train_dataset.pos_vocab)\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "n_layers = 1\n",
    "\n",
    "# Initialize the model.\n",
    "model = Tagger(input_vocab_size, output_vocab_size, n_layers)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss function weights.\n",
    "weight = torch.ones(output_vocab_size)\n",
    "weight[0] = 0\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "    \n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss(weight)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #losses.append(loss.data[0])\n",
    "        losses.append(loss.item())\n",
    "        if (i % 10) == 0:\n",
    "            # Compute dev loss over entire dev set.\n",
    "            # NOTE: This is expensive. You may want to only use a \n",
    "            # subset of the dev set.\n",
    "            #print('iteration, ', i)\n",
    "            dev_losses = []\n",
    "            for inputs, targets, lengths in dev_loader:\n",
    "                outputs, _ = model(inputs, lengths=lengths)\n",
    "                outputs = outputs.view(-1, output_vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                dev_losses.append(loss.item())\n",
    "            avg_train_loss = np.mean(losses)\n",
    "            avg_dev_loss = np.mean(dev_losses)\n",
    "            losses = []\n",
    "            #print('here')\n",
    "            print('Epoch %i Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (epoch, i, avg_train_loss, avg_dev_loss), end='\\n')\n",
    "            torch.save(model, 'pos_tagger_lstm.pt')\n",
    "        i += 1\n",
    "        \n",
    "torch.save(model, 'pos_tagger_lstm.final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09b623b-c4e3-407c-bc08-6052790330af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - 0.874188\n",
      "\n",
      "F1-scores:\n",
      "\n",
      "NOUN - 0.826618\n",
      "DET - 0.961651\n",
      "ADP - 0.169292\n",
      "PUNCT - 0.998161\n",
      "VERB - 0.766210\n",
      "PROPN - 0.703410\n",
      "_ - 0.983295\n",
      "ADJ - 0.633302\n",
      "ADV - 0.854648\n",
      "PRON - 0.871985\n",
      "CCONJ - 0.987109\n",
      "AUX - 0.887916\n",
      "SCONJ - 0.676142\n",
      "NUM - 0.706682\n",
      "SYM - 1.000000\n",
      "X - 0.000000\n",
      "INTJ - 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Collect the predictions and targets\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets, lengths in dev_loader:\n",
    "    outputs, _ = model(inputs, lengths=lengths)\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    targets = targets.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    if torch.cuda.is_available():\n",
    "        targets = targets.cpu()\n",
    "        preds = preds.cpu()\n",
    "    y_true.append(targets.data.numpy())\n",
    "    y_pred.append(preds.data.numpy())\n",
    "\n",
    "# Stack into numpy arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
    "print('Accuracy - %0.6f\\n' % acc)\n",
    "\n",
    "# Evaluate f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(y_true, y_pred, average=None)\n",
    "print('F1-scores:\\n')\n",
    "for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
    "    print('%s - %0.6f' % (label, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8947d2-0903-485f-b525-4e05c75a8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pos_tagger_lstm.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    for word, tag in zip(sentence, pos_tags):\n",
    "        print('%s - %s' % (word, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e296101-701e-44eb-8937-ca35eaf8570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_labels(sentence, labels):\n",
    "    #print(sentence)\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    print(ids)\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    #labels = [dataset.pos_vocab.id2word(x) for x in labels]\n",
    "    #sentence = [test_dataset.token_vocab.id2word(x) for x in ids]\n",
    "    for word, tag, label in zip(sentence, pos_tags, labels):\n",
    "        print('%s - %s - %s' % (word, tag, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1606b1e-fe40-4cbc-a83d-feaf4c929e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Os', 'policiais', 'federais', 'de', 'Mato', 'Grosso', 'do', 'de', 'o', 'Sul', 'entraram', 'em', 'greve', 'ontem', ',', 'em', 'adesão', 'ao', 'a', 'o', 'movimento', 'iniciado', 'no', 'em', 'o', 'Distrito', 'Federal', '.']\n",
      "[[48], [1361], [3935], [2], [3265], [3266], [11], [2], [5], [496], [3737], [7], [1219], [58], [4], [7], [3524], [31], [3], [5], [962], [6250], [22], [7], [5], [15291], [653], [6]]\n",
      "Os - SCONJ - DET\n",
      "policiais - VERB - NOUN\n",
      "federais - NOUN - ADJ\n",
      "de - ADP - ADP\n",
      "Mato - NUM - PROPN\n",
      "Grosso - ADJ - PROPN\n",
      "do - PRON - _\n",
      "de - ADP - ADP\n",
      "o - DET - DET\n",
      "Sul - NOUN - PROPN\n",
      "entraram - VERB - VERB\n",
      "em - ADP - ADP\n",
      "greve - PROPN - NOUN\n",
      "ontem - SYM - ADV\n",
      ", - ADP - PUNCT\n",
      "em - ADP - ADP\n",
      "adesão - PROPN - NOUN\n",
      "ao - ADJ - _\n",
      "a - PUNCT - ADP\n",
      "o - DET - DET\n",
      "movimento - NOUN - NOUN\n",
      "iniciado - VERB - VERB\n",
      "no - _ - _\n",
      "em - ADP - ADP\n",
      "o - DET - DET\n",
      "Distrito - NOUN - PROPN\n",
      "Federal - NOUN - PROPN\n",
      ". - PUNCT - PUNCT\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CoNLLDataset('./datasets/pt_bosque-ud-test.conllu')\n",
    "dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu')\n",
    "\n",
    "sentence, labels = test_dataset[10]\n",
    "sentence = [test_dataset.token_vocab.id2word(x) for x in sentence]\n",
    "print(sentence)\n",
    "labels = [test_dataset.pos_vocab.id2word(x) for x in labels]\n",
    "inference_with_labels(sentence, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
