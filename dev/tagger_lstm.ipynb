{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca39af7-84d8-469f-aaf6-f5764501dbdb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a53f45d-33b3-4a40-b0e2-674577ca0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beefaac-3b23-4d8a-b2d1-a337eb0e8c33",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de8dbcfa-a7fe-4393-95c6-ee59e7c0da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94ad8b-ff9e-4d97-be37-2fad7f172072",
   "metadata": {},
   "source": [
    "# CoNLLDataset e Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ef8512e-d6b2-4e96-a9c8-7fcb464af35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname, max_exs=None):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname, max_exs)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname, max_exs):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        #print(chunks)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        exs = 0\n",
    "        for chunk in chunks:\n",
    "            if not max_exs or exs < max_exs:\n",
    "                annotation = Annotation()\n",
    "                lines = chunk.split('\\n')\n",
    "                # Iterate over all lines in the chunk.\n",
    "                for line in lines:\n",
    "                    # If line is empty ignore it.\n",
    "                    if len(line)==0:\n",
    "                        continue\n",
    "                    # If line is a commend ignore it.\n",
    "                    if line[0] == '#':\n",
    "                        continue\n",
    "                    # Otherwise split on tabs and retrieve the token and the\n",
    "                    # POS tag fields.\n",
    "                    fields = line.split('\\t')\n",
    "                    annotation.tokens.append(fields[1])\n",
    "                    annotation.pos_tags.append(fields[3])\n",
    "                if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                    annotations.append(annotation)\n",
    "            exs += 1\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5907be-6c34-4e65-b922-9e814b771922",
   "metadata": {},
   "source": [
    "# Funções: pad() e collate_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22bfb3a3-52e1-4c2c-8563-723e15182391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecfc14-2652-47be-895b-3a89b6e65db9",
   "metadata": {},
   "source": [
    "# Tagger - LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09b9f91-6d8c-40b7-b21b-e8838c66e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 embedding_dim=64,\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True,\n",
    "                 pad_idx=0):\n",
    "        \"\"\"Initializes the tagger.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Size of the input vocabulary, projection\n",
    "            output_dim: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_dim: Number of units in each LSTM hidden layer.\n",
    "            bidirectional: Whether or not to use a bidirectional rnn.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_dim = input_dim \n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "          \n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "        \n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "            \n",
    "        Returns:\n",
    "            net: Probability of the next word in the sequence.\n",
    "            hidden: Hidden state of the lstm.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "        \n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            if self.bidirectional:\n",
    "                num_directions = 2\n",
    "            else:\n",
    "                num_directions = 1\n",
    "            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_dim))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        # Pack before feeding into the RNN.\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths)\n",
    "        # net, hidden = self.rnn(net, hidden) # Daniel\n",
    "        net, hidden = self.rnn(net, (hidden, hidden)) # Daniel\n",
    "        # Unpack after\n",
    "        if lengths is not None:\n",
    "            net, _ = pad_packed_sequence(net)\n",
    "        net = self.fc(net)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0c518-c0a6-42bb-bf88-c37e55c0fbd9",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020c10db-51d4-4bec-8ba7-829d3bf554af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0 - Train Loss: 2.956676 - Dev Loss: 2.927128\n",
      "Epoch 0 Iteration 10 - Train Loss: 2.841085 - Dev Loss: 2.726756\n",
      "Epoch 0 Iteration 20 - Train Loss: 2.618679 - Dev Loss: 2.459507\n",
      "Epoch 0 Iteration 30 - Train Loss: 2.316849 - Dev Loss: 2.137840\n",
      "Epoch 0 Iteration 40 - Train Loss: 2.027229 - Dev Loss: 1.919592\n",
      "Epoch 0 Iteration 50 - Train Loss: 1.856944 - Dev Loss: 1.733299\n",
      "Epoch 0 Iteration 60 - Train Loss: 1.684162 - Dev Loss: 1.562637\n",
      "Epoch 0 Iteration 70 - Train Loss: 1.529749 - Dev Loss: 1.434664\n",
      "Epoch 0 Iteration 80 - Train Loss: 1.388082 - Dev Loss: 1.327475\n",
      "Epoch 0 Iteration 90 - Train Loss: 1.304766 - Dev Loss: 1.246329\n",
      "Epoch 0 Iteration 100 - Train Loss: 1.197614 - Dev Loss: 1.181075\n",
      "Epoch 0 Iteration 110 - Train Loss: 1.165108 - Dev Loss: 1.125966\n",
      "Epoch 0 Iteration 120 - Train Loss: 1.184885 - Dev Loss: 1.087418\n",
      "Epoch 0 Iteration 130 - Train Loss: 1.118506 - Dev Loss: 1.046880\n",
      "Epoch 0 Iteration 140 - Train Loss: 1.079871 - Dev Loss: 1.013741\n",
      "Epoch 0 Iteration 150 - Train Loss: 1.015714 - Dev Loss: 0.985337\n",
      "Epoch 0 Iteration 160 - Train Loss: 0.994159 - Dev Loss: 0.958118\n",
      "Epoch 0 Iteration 170 - Train Loss: 0.958874 - Dev Loss: 0.935502\n",
      "Epoch 0 Iteration 180 - Train Loss: 0.941367 - Dev Loss: 0.914694\n",
      "Epoch 0 Iteration 190 - Train Loss: 0.893676 - Dev Loss: 0.893170\n",
      "Epoch 0 Iteration 200 - Train Loss: 0.919450 - Dev Loss: 0.874749\n",
      "Epoch 0 Iteration 210 - Train Loss: 0.922914 - Dev Loss: 0.859118\n",
      "Epoch 0 Iteration 220 - Train Loss: 0.896863 - Dev Loss: 0.850033\n",
      "Epoch 0 Iteration 230 - Train Loss: 0.896414 - Dev Loss: 0.832873\n",
      "Epoch 0 Iteration 240 - Train Loss: 0.851852 - Dev Loss: 0.817898\n",
      "Epoch 0 Iteration 250 - Train Loss: 0.860137 - Dev Loss: 0.805810\n",
      "Epoch 1 Iteration 260 - Train Loss: 0.836457 - Dev Loss: 0.788914\n",
      "Epoch 1 Iteration 270 - Train Loss: 0.818231 - Dev Loss: 0.775486\n",
      "Epoch 1 Iteration 280 - Train Loss: 0.772702 - Dev Loss: 0.764741\n",
      "Epoch 1 Iteration 290 - Train Loss: 0.758638 - Dev Loss: 0.770888\n",
      "Epoch 1 Iteration 300 - Train Loss: 0.771231 - Dev Loss: 0.746654\n",
      "Epoch 1 Iteration 310 - Train Loss: 0.756385 - Dev Loss: 0.734219\n",
      "Epoch 1 Iteration 320 - Train Loss: 0.708652 - Dev Loss: 0.733853\n",
      "Epoch 1 Iteration 330 - Train Loss: 0.758220 - Dev Loss: 0.721711\n",
      "Epoch 1 Iteration 340 - Train Loss: 0.715128 - Dev Loss: 0.718167\n",
      "Epoch 1 Iteration 350 - Train Loss: 0.706391 - Dev Loss: 0.707951\n",
      "Epoch 1 Iteration 360 - Train Loss: 0.738432 - Dev Loss: 0.696688\n",
      "Epoch 1 Iteration 370 - Train Loss: 0.739250 - Dev Loss: 0.690728\n",
      "Epoch 1 Iteration 380 - Train Loss: 0.717214 - Dev Loss: 0.684097\n",
      "Epoch 1 Iteration 390 - Train Loss: 0.668014 - Dev Loss: 0.673162\n",
      "Epoch 1 Iteration 400 - Train Loss: 0.711113 - Dev Loss: 0.669636\n",
      "Epoch 1 Iteration 410 - Train Loss: 0.673908 - Dev Loss: 0.664049\n",
      "Epoch 1 Iteration 420 - Train Loss: 0.680798 - Dev Loss: 0.653422\n",
      "Epoch 1 Iteration 430 - Train Loss: 0.720752 - Dev Loss: 0.649758\n",
      "Epoch 1 Iteration 440 - Train Loss: 0.642911 - Dev Loss: 0.643695\n",
      "Epoch 1 Iteration 450 - Train Loss: 0.698421 - Dev Loss: 0.638454\n",
      "Epoch 1 Iteration 460 - Train Loss: 0.643244 - Dev Loss: 0.630652\n",
      "Epoch 1 Iteration 470 - Train Loss: 0.619703 - Dev Loss: 0.620668\n",
      "Epoch 1 Iteration 480 - Train Loss: 0.669147 - Dev Loss: 0.623039\n",
      "Epoch 1 Iteration 490 - Train Loss: 0.648937 - Dev Loss: 0.614636\n",
      "Epoch 1 Iteration 500 - Train Loss: 0.644408 - Dev Loss: 0.614156\n",
      "Epoch 1 Iteration 510 - Train Loss: 0.625558 - Dev Loss: 0.604621\n",
      "Epoch 2 Iteration 520 - Train Loss: 0.605479 - Dev Loss: 0.595803\n",
      "Epoch 2 Iteration 530 - Train Loss: 0.586647 - Dev Loss: 0.593458\n",
      "Epoch 2 Iteration 540 - Train Loss: 0.567962 - Dev Loss: 0.594441\n",
      "Epoch 2 Iteration 550 - Train Loss: 0.569187 - Dev Loss: 0.587398\n",
      "Epoch 2 Iteration 560 - Train Loss: 0.573039 - Dev Loss: 0.584796\n",
      "Epoch 2 Iteration 570 - Train Loss: 0.586645 - Dev Loss: 0.583577\n",
      "Epoch 2 Iteration 580 - Train Loss: 0.599024 - Dev Loss: 0.575283\n",
      "Epoch 2 Iteration 590 - Train Loss: 0.580813 - Dev Loss: 0.569095\n",
      "Epoch 2 Iteration 600 - Train Loss: 0.594086 - Dev Loss: 0.567231\n",
      "Epoch 2 Iteration 610 - Train Loss: 0.574446 - Dev Loss: 0.566576\n",
      "Epoch 2 Iteration 620 - Train Loss: 0.525717 - Dev Loss: 0.565615\n",
      "Epoch 2 Iteration 630 - Train Loss: 0.534424 - Dev Loss: 0.557399\n",
      "Epoch 2 Iteration 640 - Train Loss: 0.585700 - Dev Loss: 0.552495\n",
      "Epoch 2 Iteration 650 - Train Loss: 0.540398 - Dev Loss: 0.547393\n",
      "Epoch 2 Iteration 660 - Train Loss: 0.497358 - Dev Loss: 0.549233\n",
      "Epoch 2 Iteration 670 - Train Loss: 0.497701 - Dev Loss: 0.538070\n",
      "Epoch 2 Iteration 680 - Train Loss: 0.547362 - Dev Loss: 0.534278\n",
      "Epoch 2 Iteration 690 - Train Loss: 0.512534 - Dev Loss: 0.537390\n",
      "Epoch 2 Iteration 700 - Train Loss: 0.521703 - Dev Loss: 0.530846\n",
      "Epoch 2 Iteration 710 - Train Loss: 0.521940 - Dev Loss: 0.528205\n",
      "Epoch 2 Iteration 720 - Train Loss: 0.504384 - Dev Loss: 0.531218\n",
      "Epoch 2 Iteration 730 - Train Loss: 0.514046 - Dev Loss: 0.520216\n",
      "Epoch 2 Iteration 740 - Train Loss: 0.551365 - Dev Loss: 0.523777\n",
      "Epoch 2 Iteration 750 - Train Loss: 0.525243 - Dev Loss: 0.519755\n",
      "Epoch 2 Iteration 760 - Train Loss: 0.489074 - Dev Loss: 0.516871\n",
      "Epoch 3 Iteration 770 - Train Loss: 0.496013 - Dev Loss: 0.508899\n",
      "Epoch 3 Iteration 780 - Train Loss: 0.446319 - Dev Loss: 0.506568\n",
      "Epoch 3 Iteration 790 - Train Loss: 0.453932 - Dev Loss: 0.505192\n",
      "Epoch 3 Iteration 800 - Train Loss: 0.421876 - Dev Loss: 0.506919\n",
      "Epoch 3 Iteration 810 - Train Loss: 0.482957 - Dev Loss: 0.503239\n",
      "Epoch 3 Iteration 820 - Train Loss: 0.448523 - Dev Loss: 0.501900\n",
      "Epoch 3 Iteration 830 - Train Loss: 0.452307 - Dev Loss: 0.493186\n",
      "Epoch 3 Iteration 840 - Train Loss: 0.420766 - Dev Loss: 0.505341\n",
      "Epoch 3 Iteration 850 - Train Loss: 0.462991 - Dev Loss: 0.487192\n",
      "Epoch 3 Iteration 860 - Train Loss: 0.463459 - Dev Loss: 0.492779\n",
      "Epoch 3 Iteration 870 - Train Loss: 0.466514 - Dev Loss: 0.487854\n",
      "Epoch 3 Iteration 880 - Train Loss: 0.452515 - Dev Loss: 0.479102\n",
      "Epoch 3 Iteration 890 - Train Loss: 0.450461 - Dev Loss: 0.491323\n",
      "Epoch 3 Iteration 900 - Train Loss: 0.435759 - Dev Loss: 0.479844\n",
      "Epoch 3 Iteration 910 - Train Loss: 0.427100 - Dev Loss: 0.481844\n",
      "Epoch 3 Iteration 920 - Train Loss: 0.469572 - Dev Loss: 0.471757\n",
      "Epoch 3 Iteration 930 - Train Loss: 0.399677 - Dev Loss: 0.473415\n",
      "Epoch 3 Iteration 940 - Train Loss: 0.431880 - Dev Loss: 0.472602\n",
      "Epoch 3 Iteration 950 - Train Loss: 0.457884 - Dev Loss: 0.466189\n",
      "Epoch 3 Iteration 960 - Train Loss: 0.412970 - Dev Loss: 0.468074\n",
      "Epoch 3 Iteration 970 - Train Loss: 0.439513 - Dev Loss: 0.466987\n",
      "Epoch 3 Iteration 980 - Train Loss: 0.425156 - Dev Loss: 0.458086\n",
      "Epoch 3 Iteration 990 - Train Loss: 0.407278 - Dev Loss: 0.459418\n",
      "Epoch 3 Iteration 1000 - Train Loss: 0.386450 - Dev Loss: 0.461725\n",
      "Epoch 3 Iteration 1010 - Train Loss: 0.419492 - Dev Loss: 0.463570\n",
      "Epoch 3 Iteration 1020 - Train Loss: 0.428193 - Dev Loss: 0.450853\n",
      "Epoch 4 Iteration 1030 - Train Loss: 0.370106 - Dev Loss: 0.447935\n",
      "Epoch 4 Iteration 1040 - Train Loss: 0.376350 - Dev Loss: 0.451026\n",
      "Epoch 4 Iteration 1050 - Train Loss: 0.392953 - Dev Loss: 0.445613\n",
      "Epoch 4 Iteration 1060 - Train Loss: 0.365005 - Dev Loss: 0.446321\n",
      "Epoch 4 Iteration 1070 - Train Loss: 0.377529 - Dev Loss: 0.444519\n",
      "Epoch 4 Iteration 1080 - Train Loss: 0.387930 - Dev Loss: 0.449447\n",
      "Epoch 4 Iteration 1090 - Train Loss: 0.349704 - Dev Loss: 0.441675\n",
      "Epoch 4 Iteration 1100 - Train Loss: 0.361575 - Dev Loss: 0.439109\n",
      "Epoch 4 Iteration 1110 - Train Loss: 0.379787 - Dev Loss: 0.438050\n",
      "Epoch 4 Iteration 1120 - Train Loss: 0.376113 - Dev Loss: 0.436514\n",
      "Epoch 4 Iteration 1130 - Train Loss: 0.367666 - Dev Loss: 0.444992\n",
      "Epoch 4 Iteration 1140 - Train Loss: 0.351741 - Dev Loss: 0.436452\n",
      "Epoch 4 Iteration 1150 - Train Loss: 0.354953 - Dev Loss: 0.434894\n",
      "Epoch 4 Iteration 1160 - Train Loss: 0.344957 - Dev Loss: 0.435118\n",
      "Epoch 4 Iteration 1170 - Train Loss: 0.347939 - Dev Loss: 0.438852\n",
      "Epoch 4 Iteration 1180 - Train Loss: 0.350466 - Dev Loss: 0.432309\n",
      "Epoch 4 Iteration 1190 - Train Loss: 0.351462 - Dev Loss: 0.430654\n",
      "Epoch 4 Iteration 1200 - Train Loss: 0.334513 - Dev Loss: 0.438620\n",
      "Epoch 4 Iteration 1210 - Train Loss: 0.353933 - Dev Loss: 0.422023\n",
      "Epoch 4 Iteration 1220 - Train Loss: 0.384095 - Dev Loss: 0.434240\n",
      "Epoch 4 Iteration 1230 - Train Loss: 0.331800 - Dev Loss: 0.418111\n",
      "Epoch 4 Iteration 1240 - Train Loss: 0.327795 - Dev Loss: 0.416312\n",
      "Epoch 4 Iteration 1250 - Train Loss: 0.367174 - Dev Loss: 0.423887\n",
      "Epoch 4 Iteration 1260 - Train Loss: 0.342655 - Dev Loss: 0.419747\n",
      "Epoch 4 Iteration 1270 - Train Loss: 0.340484 - Dev Loss: 0.412558\n",
      "Epoch 5 Iteration 1280 - Train Loss: 0.349779 - Dev Loss: 0.416962\n",
      "Epoch 5 Iteration 1290 - Train Loss: 0.283352 - Dev Loss: 0.414435\n",
      "Epoch 5 Iteration 1300 - Train Loss: 0.331586 - Dev Loss: 0.422648\n",
      "Epoch 5 Iteration 1310 - Train Loss: 0.311322 - Dev Loss: 0.412608\n",
      "Epoch 5 Iteration 1320 - Train Loss: 0.278343 - Dev Loss: 0.420465\n",
      "Epoch 5 Iteration 1330 - Train Loss: 0.316754 - Dev Loss: 0.404923\n",
      "Epoch 5 Iteration 1340 - Train Loss: 0.259028 - Dev Loss: 0.409754\n",
      "Epoch 5 Iteration 1350 - Train Loss: 0.305011 - Dev Loss: 0.407313\n",
      "Epoch 5 Iteration 1360 - Train Loss: 0.306755 - Dev Loss: 0.406511\n",
      "Epoch 5 Iteration 1370 - Train Loss: 0.310273 - Dev Loss: 0.408297\n",
      "Epoch 5 Iteration 1380 - Train Loss: 0.302310 - Dev Loss: 0.400877\n",
      "Epoch 5 Iteration 1390 - Train Loss: 0.260523 - Dev Loss: 0.407884\n",
      "Epoch 5 Iteration 1400 - Train Loss: 0.288504 - Dev Loss: 0.413317\n",
      "Epoch 5 Iteration 1410 - Train Loss: 0.290357 - Dev Loss: 0.418430\n",
      "Epoch 5 Iteration 1420 - Train Loss: 0.275668 - Dev Loss: 0.405537\n",
      "Epoch 5 Iteration 1430 - Train Loss: 0.279365 - Dev Loss: 0.409800\n",
      "Epoch 5 Iteration 1440 - Train Loss: 0.292942 - Dev Loss: 0.401032\n",
      "Epoch 5 Iteration 1450 - Train Loss: 0.294100 - Dev Loss: 0.404260\n",
      "Epoch 5 Iteration 1460 - Train Loss: 0.265015 - Dev Loss: 0.393435\n",
      "Epoch 5 Iteration 1470 - Train Loss: 0.299850 - Dev Loss: 0.407239\n",
      "Epoch 5 Iteration 1480 - Train Loss: 0.289716 - Dev Loss: 0.399487\n",
      "Epoch 5 Iteration 1490 - Train Loss: 0.284240 - Dev Loss: 0.411575\n",
      "Epoch 5 Iteration 1500 - Train Loss: 0.315673 - Dev Loss: 0.397986\n",
      "Epoch 5 Iteration 1510 - Train Loss: 0.303238 - Dev Loss: 0.411555\n",
      "Epoch 5 Iteration 1520 - Train Loss: 0.285636 - Dev Loss: 0.395523\n",
      "Epoch 5 Iteration 1530 - Train Loss: 0.303194 - Dev Loss: 0.399254\n"
     ]
    }
   ],
   "source": [
    "# Load datasets.\n",
    "train_dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu', 4096)\n",
    "dev_dataset = CoNLLDataset('./datasets/pt_bosque-ud-dev.conllu', 1024)\n",
    "\n",
    "dev_dataset.token_vocab = train_dataset.token_vocab\n",
    "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(train_dataset.token_vocab)\n",
    "output_vocab_size = len(train_dataset.pos_vocab)\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "n_layers = 1\n",
    "\n",
    "# Initialize the model.\n",
    "model = Tagger(input_vocab_size, output_vocab_size, n_layers)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss function weights.\n",
    "weight = torch.ones(output_vocab_size)\n",
    "weight[0] = 0\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "    \n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss(weight)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #losses.append(loss.data[0])\n",
    "        losses.append(loss.item())\n",
    "        if (i % 10) == 0:\n",
    "            # Compute dev loss over entire dev set.\n",
    "            # NOTE: This is expensive. You may want to only use a \n",
    "            # subset of the dev set.\n",
    "            #print('iteration, ', i)\n",
    "            dev_losses = []\n",
    "            for inputs, targets, lengths in dev_loader:\n",
    "                outputs, _ = model(inputs, lengths=lengths)\n",
    "                outputs = outputs.view(-1, output_vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                dev_losses.append(loss.item())\n",
    "            avg_train_loss = np.mean(losses)\n",
    "            avg_dev_loss = np.mean(dev_losses)\n",
    "            losses = []\n",
    "            #print('here')\n",
    "            print('Epoch %i Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (epoch, i, avg_train_loss, avg_dev_loss), end='\\n')\n",
    "            torch.save(model, 'pos_tagger_lstm.pt')\n",
    "        i += 1\n",
    "        \n",
    "torch.save(model, 'pos_tagger_lstm.final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a09b623b-c4e3-407c-bc08-6052790330af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - 0.871766\n",
      "\n",
      "F1-scores:\n",
      "\n",
      "NOUN - 0.827912\n",
      "DET - 0.962137\n",
      "ADP - 0.960179\n",
      "PUNCT - 0.991180\n",
      "VERB - 0.093047\n",
      "PROPN - 0.691165\n",
      "_ - 0.989118\n",
      "ADJ - 0.599366\n",
      "ADV - 0.837179\n",
      "PRON - 0.870968\n",
      "CCONJ - 0.989880\n",
      "AUX - 0.883186\n",
      "SCONJ - 0.701719\n",
      "NUM - 0.758962\n",
      "SYM - 1.000000\n",
      "X - 0.000000\n",
      "INTJ - 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Collect the predictions and targets\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets, lengths in dev_loader:\n",
    "    outputs, _ = model(inputs, lengths=lengths)\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    targets = targets.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    if torch.cuda.is_available():\n",
    "        targets = targets.cpu()\n",
    "        preds = preds.cpu()\n",
    "    y_true.append(targets.data.numpy())\n",
    "    y_pred.append(preds.data.numpy())\n",
    "\n",
    "# Stack into numpy arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
    "print('Accuracy - %0.6f\\n' % acc)\n",
    "\n",
    "# Evaluate f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(y_true, y_pred, average=None)\n",
    "print('F1-scores:\\n')\n",
    "for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
    "    print('%s - %0.6f' % (label, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9a8947d2-0903-485f-b525-4e05c75a8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pos_tagger_lstm.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    for word, tag in zip(sentence, pos_tags):\n",
    "        print('%s - %s' % (word, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e296101-701e-44eb-8937-ca35eaf8570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_labels(sentence, labels):\n",
    "    #print(sentence)\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    print(ids)\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    #labels = [dataset.pos_vocab.id2word(x) for x in labels]\n",
    "    #sentence = [test_dataset.token_vocab.id2word(x) for x in ids]\n",
    "    for word, tag, label in zip(sentence, pos_tags, labels):\n",
    "        print('%s - %s - %s' % (word, tag, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1606b1e-fe40-4cbc-a83d-feaf4c929e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Os', 'policiais', 'federais', 'de', 'Mato', 'Grosso', 'do', 'de', 'o', 'Sul', 'entraram', 'em', 'greve', 'ontem', ',', 'em', 'adesão', 'ao', 'a', 'o', 'movimento', 'iniciado', 'no', 'em', 'o', 'Distrito', 'Federal', '.']\n",
      "[[48], [1361], [3935], [2], [3265], [3266], [11], [2], [5], [496], [3737], [7], [1219], [58], [4], [7], [3524], [31], [3], [5], [962], [6250], [22], [7], [5], [15291], [653], [6]]\n",
      "Os - SCONJ - DET\n",
      "policiais - VERB - NOUN\n",
      "federais - PROPN - ADJ\n",
      "de - ADP - ADP\n",
      "Mato - NOUN - PROPN\n",
      "Grosso - VERB - PROPN\n",
      "do - _ - _\n",
      "de - ADP - ADP\n",
      "o - DET - DET\n",
      "Sul - NOUN - PROPN\n",
      "entraram - ADJ - VERB\n",
      "em - ADP - ADP\n",
      "greve - NUM - NOUN\n",
      "ontem - SYM - ADV\n",
      ", - PRON - PUNCT\n",
      "em - ADP - ADP\n",
      "adesão - NOUN - NOUN\n",
      "ao - ADJ - _\n",
      "a - PUNCT - ADP\n",
      "o - DET - DET\n",
      "movimento - NOUN - NOUN\n",
      "iniciado - VERB - VERB\n",
      "no - _ - _\n",
      "em - ADP - ADP\n",
      "o - DET - DET\n",
      "Distrito - NOUN - PROPN\n",
      "Federal - PROPN - PROPN\n",
      ". - PUNCT - PUNCT\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CoNLLDataset('./datasets/pt_bosque-ud-test.conllu')\n",
    "dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu')\n",
    "\n",
    "sentence, labels = test_dataset[10]\n",
    "sentence = [test_dataset.token_vocab.id2word(x) for x in sentence]\n",
    "print(sentence)\n",
    "labels = [test_dataset.pos_vocab.id2word(x) for x in labels]\n",
    "inference_with_labels(sentence, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
