{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca39af7-84d8-469f-aaf6-f5764501dbdb",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a53f45d-33b3-4a40-b0e2-674577ca0149",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc157566-3db0-4482-92bb-726d624bc86e",
   "metadata": {},
   "source": [
    "# Train Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f04f6eae-e285-47f4-b463-172ebc32624e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv( \n",
    "#     './datasets/pt_bosque-ud-train--pos.conllu',\n",
    "#     sep = '\\t' )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfa1169a-652f-40d3-a24a-4ab70cf040d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train = df[ df.iloc[:,3] != '_' ]\n",
    "# df_pos_train = df_pos_train[ [ '_.1', '_.3' ] ]\n",
    "# df_pos_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bc03a4-4e6f-466e-b704-756cc2a76718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bd289cb-2489-45b5-84de-0b50f9a8e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42277ed-49e8-4793-8a0a-b8019fb079cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_train.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efa12d98-6f6d-4c09-ad93-8f373ac3acf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_train.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af9c142-4e59-458a-9d67-dffa944f7f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_train.iloc[:, 0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3ed17-26ba-4b8b-9de9-308603cf9468",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Dev Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcfd7969-20c1-41e1-8ed1-a8f045acfada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv( \n",
    "#     './datasets/pt_bosque-ud-dev--pos.conllu',\n",
    "#     sep = '\\t' )\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64eb7acd-da9f-4203-9b4e-1a56cc2a2244",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev = df[ df.iloc[:,3] != '_' ]\n",
    "# df_pos_dev = df_pos_dev[ [ '_.1', '_.3' ] ]\n",
    "# df_pos_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c600b44-4e82-41cd-8c6e-15c117757340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cddb54a2-c835-48b8-8631-b9535b9af432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d9a82cb-0c19-42c4-852c-692b0229607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_dev.iloc[:, 1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03645fb0-1042-414e-af3e-4f8a3f5976ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pos_dev.iloc[:, 0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6793d8b2-3aa4-457d-ab28-b8cc035cb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df_pos_dev.iloc[:, 0].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4beefaac-3b23-4d8a-b2d1-a337eb0e8c33",
   "metadata": {},
   "source": [
    "# Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de8dbcfa-a7fe-4393-95c6-ee59e7c0da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e94ad8b-ff9e-4d97-be37-2fad7f172072",
   "metadata": {},
   "source": [
    "# CoNLLDataset e Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ef8512e-d6b2-4e96-a9c8-7fcb464af35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname, max_exs=None):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname, max_exs)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname, max_exs):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        #print(chunks)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        exs = 0\n",
    "        for chunk in chunks:\n",
    "            if not max_exs or exs < max_exs:\n",
    "                annotation = Annotation()\n",
    "                lines = chunk.split('\\n')\n",
    "                # Iterate over all lines in the chunk.\n",
    "                for line in lines:\n",
    "                    # If line is empty ignore it.\n",
    "                    if len(line)==0:\n",
    "                        continue\n",
    "                    # If line is a commend ignore it.\n",
    "                    if line[0] == '#':\n",
    "                        continue\n",
    "                    # Otherwise split on tabs and retrieve the token and the\n",
    "                    # POS tag fields.\n",
    "                    fields = line.split('\\t')\n",
    "                    annotation.tokens.append(fields[1])\n",
    "                    annotation.pos_tags.append(fields[3])\n",
    "                if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                    annotations.append(annotation)\n",
    "            exs += 1\n",
    "        return annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5907be-6c34-4e65-b922-9e814b771922",
   "metadata": {},
   "source": [
    "# Funções: pad() e collate_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22bfb3a3-52e1-4c2c-8563-723e15182391",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdecfc14-2652-47be-895b-3a89b6e65db9",
   "metadata": {},
   "source": [
    "# Tagger - GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e09b9f91-6d8c-40b7-b21b-e8838c66e419",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim,\n",
    "                 output_dim,\n",
    "                 n_layers, \n",
    "                 embedding_dim=64,\n",
    "                 hidden_dim=64,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True,\n",
    "                 pad_idx=0):\n",
    "        \"\"\"Initializes the tagger.\n",
    "        \n",
    "        Args:\n",
    "            input_dim: Size of the input vocabulary, projection\n",
    "            output_dim: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_dim: Number of units in each LSTM hidden layer.\n",
    "            bidirectional: Whether or not to use a bidirectional rnn.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_dim = input_dim \n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.bidirectional = bidirectional\n",
    "          \n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers = n_layers, \n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout = dropout if n_layers > 1 else 0)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "        \n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "            \n",
    "        Returns:\n",
    "            net: Probability of the next word in the sequence.\n",
    "            hidden: Hidden state of the lstm.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "        \n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            if self.bidirectional:\n",
    "                num_directions = 2\n",
    "            else:\n",
    "                num_directions = 1\n",
    "            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_dim))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        # Pack before feeding into the RNN.\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths)\n",
    "        net, hidden = self.rnn(net, hidden)\n",
    "        # Unpack after\n",
    "        if lengths is not None:\n",
    "            net, _ = pad_packed_sequence(net)\n",
    "        net = self.fc(net)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d0c518-c0a6-42bb-bf88-c37e55c0fbd9",
   "metadata": {},
   "source": [
    "# Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "020c10db-51d4-4bec-8ba7-829d3bf554af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Iteration 0 - Train Loss: 2.915912 - Dev Loss: 2.903633\n",
      "Epoch 0 Iteration 10 - Train Loss: 2.729256 - Dev Loss: 2.539190\n",
      "Epoch 0 Iteration 20 - Train Loss: 2.388396 - Dev Loss: 2.194318\n",
      "Epoch 0 Iteration 30 - Train Loss: 2.046659 - Dev Loss: 1.934616\n",
      "Epoch 0 Iteration 40 - Train Loss: 1.863444 - Dev Loss: 1.732096\n",
      "Epoch 0 Iteration 50 - Train Loss: 1.632154 - Dev Loss: 1.555162\n",
      "Epoch 0 Iteration 60 - Train Loss: 1.502246 - Dev Loss: 1.409319\n",
      "Epoch 0 Iteration 70 - Train Loss: 1.343846 - Dev Loss: 1.298839\n",
      "Epoch 0 Iteration 80 - Train Loss: 1.301471 - Dev Loss: 1.211881\n",
      "Epoch 0 Iteration 90 - Train Loss: 1.180121 - Dev Loss: 1.146245\n",
      "Epoch 0 Iteration 100 - Train Loss: 1.130771 - Dev Loss: 1.095716\n",
      "Epoch 0 Iteration 110 - Train Loss: 1.080941 - Dev Loss: 1.051826\n",
      "Epoch 0 Iteration 120 - Train Loss: 1.040025 - Dev Loss: 1.016077\n",
      "Epoch 0 Iteration 130 - Train Loss: 1.047148 - Dev Loss: 0.980484\n",
      "Epoch 0 Iteration 140 - Train Loss: 0.981174 - Dev Loss: 0.952776\n",
      "Epoch 0 Iteration 150 - Train Loss: 0.937855 - Dev Loss: 0.932576\n",
      "Epoch 0 Iteration 160 - Train Loss: 0.919122 - Dev Loss: 0.910161\n",
      "Epoch 0 Iteration 170 - Train Loss: 0.925117 - Dev Loss: 0.897850\n",
      "Epoch 0 Iteration 180 - Train Loss: 0.918515 - Dev Loss: 0.875084\n",
      "Epoch 0 Iteration 190 - Train Loss: 0.925198 - Dev Loss: 0.858699\n",
      "Epoch 0 Iteration 200 - Train Loss: 0.853156 - Dev Loss: 0.845832\n",
      "Epoch 0 Iteration 210 - Train Loss: 0.920787 - Dev Loss: 0.836802\n",
      "Epoch 0 Iteration 220 - Train Loss: 0.865581 - Dev Loss: 0.822100\n",
      "Epoch 0 Iteration 230 - Train Loss: 0.901725 - Dev Loss: 0.807241\n",
      "Epoch 0 Iteration 240 - Train Loss: 0.829201 - Dev Loss: 0.798470\n",
      "Epoch 0 Iteration 250 - Train Loss: 0.842421 - Dev Loss: 0.789178\n",
      "Epoch 1 Iteration 260 - Train Loss: 0.786458 - Dev Loss: 0.776079\n",
      "Epoch 1 Iteration 270 - Train Loss: 0.725117 - Dev Loss: 0.765074\n",
      "Epoch 1 Iteration 280 - Train Loss: 0.741720 - Dev Loss: 0.755694\n",
      "Epoch 1 Iteration 290 - Train Loss: 0.747919 - Dev Loss: 0.745248\n",
      "Epoch 1 Iteration 300 - Train Loss: 0.775505 - Dev Loss: 0.741653\n",
      "Epoch 1 Iteration 310 - Train Loss: 0.730343 - Dev Loss: 0.732381\n",
      "Epoch 1 Iteration 320 - Train Loss: 0.742284 - Dev Loss: 0.724015\n",
      "Epoch 1 Iteration 330 - Train Loss: 0.738726 - Dev Loss: 0.714402\n",
      "Epoch 1 Iteration 340 - Train Loss: 0.706096 - Dev Loss: 0.706431\n",
      "Epoch 1 Iteration 350 - Train Loss: 0.757129 - Dev Loss: 0.700881\n",
      "Epoch 1 Iteration 360 - Train Loss: 0.691846 - Dev Loss: 0.696538\n",
      "Epoch 1 Iteration 370 - Train Loss: 0.689316 - Dev Loss: 0.688110\n",
      "Epoch 1 Iteration 380 - Train Loss: 0.708339 - Dev Loss: 0.678120\n",
      "Epoch 1 Iteration 390 - Train Loss: 0.685901 - Dev Loss: 0.670511\n",
      "Epoch 1 Iteration 400 - Train Loss: 0.699606 - Dev Loss: 0.665331\n",
      "Epoch 1 Iteration 410 - Train Loss: 0.683042 - Dev Loss: 0.659016\n",
      "Epoch 1 Iteration 420 - Train Loss: 0.672454 - Dev Loss: 0.651777\n",
      "Epoch 1 Iteration 430 - Train Loss: 0.689200 - Dev Loss: 0.644571\n",
      "Epoch 1 Iteration 440 - Train Loss: 0.639448 - Dev Loss: 0.638668\n",
      "Epoch 1 Iteration 450 - Train Loss: 0.629951 - Dev Loss: 0.629641\n",
      "Epoch 1 Iteration 460 - Train Loss: 0.664253 - Dev Loss: 0.624746\n",
      "Epoch 1 Iteration 470 - Train Loss: 0.622044 - Dev Loss: 0.622228\n",
      "Epoch 1 Iteration 480 - Train Loss: 0.630159 - Dev Loss: 0.617447\n",
      "Epoch 1 Iteration 490 - Train Loss: 0.701624 - Dev Loss: 0.612407\n",
      "Epoch 1 Iteration 500 - Train Loss: 0.581016 - Dev Loss: 0.605789\n",
      "Epoch 1 Iteration 510 - Train Loss: 0.615220 - Dev Loss: 0.603019\n",
      "Epoch 2 Iteration 520 - Train Loss: 0.626548 - Dev Loss: 0.599562\n",
      "Epoch 2 Iteration 530 - Train Loss: 0.637958 - Dev Loss: 0.593514\n",
      "Epoch 2 Iteration 540 - Train Loss: 0.564273 - Dev Loss: 0.586196\n",
      "Epoch 2 Iteration 550 - Train Loss: 0.567697 - Dev Loss: 0.585647\n",
      "Epoch 2 Iteration 560 - Train Loss: 0.559835 - Dev Loss: 0.578718\n",
      "Epoch 2 Iteration 570 - Train Loss: 0.568034 - Dev Loss: 0.576685\n",
      "Epoch 2 Iteration 580 - Train Loss: 0.539523 - Dev Loss: 0.571440\n",
      "Epoch 2 Iteration 590 - Train Loss: 0.563072 - Dev Loss: 0.566322\n",
      "Epoch 2 Iteration 600 - Train Loss: 0.570215 - Dev Loss: 0.564840\n",
      "Epoch 2 Iteration 610 - Train Loss: 0.506121 - Dev Loss: 0.560531\n",
      "Epoch 2 Iteration 620 - Train Loss: 0.564991 - Dev Loss: 0.554488\n",
      "Epoch 2 Iteration 630 - Train Loss: 0.541074 - Dev Loss: 0.556157\n",
      "Epoch 2 Iteration 640 - Train Loss: 0.509020 - Dev Loss: 0.548745\n",
      "Epoch 2 Iteration 650 - Train Loss: 0.549104 - Dev Loss: 0.547594\n",
      "Epoch 2 Iteration 660 - Train Loss: 0.548546 - Dev Loss: 0.541644\n",
      "Epoch 2 Iteration 670 - Train Loss: 0.527301 - Dev Loss: 0.535269\n",
      "Epoch 2 Iteration 680 - Train Loss: 0.492437 - Dev Loss: 0.531940\n",
      "Epoch 2 Iteration 690 - Train Loss: 0.513890 - Dev Loss: 0.530062\n",
      "Epoch 2 Iteration 700 - Train Loss: 0.522414 - Dev Loss: 0.526417\n",
      "Epoch 2 Iteration 710 - Train Loss: 0.506222 - Dev Loss: 0.523370\n",
      "Epoch 2 Iteration 720 - Train Loss: 0.510085 - Dev Loss: 0.520814\n",
      "Epoch 2 Iteration 730 - Train Loss: 0.500954 - Dev Loss: 0.520406\n",
      "Epoch 2 Iteration 740 - Train Loss: 0.487988 - Dev Loss: 0.513419\n",
      "Epoch 2 Iteration 750 - Train Loss: 0.533923 - Dev Loss: 0.511008\n",
      "Epoch 2 Iteration 760 - Train Loss: 0.467944 - Dev Loss: 0.507029\n",
      "Epoch 3 Iteration 770 - Train Loss: 0.468291 - Dev Loss: 0.504290\n",
      "Epoch 3 Iteration 780 - Train Loss: 0.444181 - Dev Loss: 0.503190\n",
      "Epoch 3 Iteration 790 - Train Loss: 0.456208 - Dev Loss: 0.495810\n",
      "Epoch 3 Iteration 800 - Train Loss: 0.485175 - Dev Loss: 0.495036\n",
      "Epoch 3 Iteration 810 - Train Loss: 0.462285 - Dev Loss: 0.493443\n",
      "Epoch 3 Iteration 820 - Train Loss: 0.450717 - Dev Loss: 0.489265\n",
      "Epoch 3 Iteration 830 - Train Loss: 0.443241 - Dev Loss: 0.483066\n",
      "Epoch 3 Iteration 840 - Train Loss: 0.447309 - Dev Loss: 0.482013\n",
      "Epoch 3 Iteration 850 - Train Loss: 0.448039 - Dev Loss: 0.487606\n",
      "Epoch 3 Iteration 860 - Train Loss: 0.421649 - Dev Loss: 0.480345\n",
      "Epoch 3 Iteration 870 - Train Loss: 0.433795 - Dev Loss: 0.476923\n",
      "Epoch 3 Iteration 880 - Train Loss: 0.399127 - Dev Loss: 0.472786\n",
      "Epoch 3 Iteration 890 - Train Loss: 0.437747 - Dev Loss: 0.474005\n",
      "Epoch 3 Iteration 900 - Train Loss: 0.442135 - Dev Loss: 0.471308\n",
      "Epoch 3 Iteration 910 - Train Loss: 0.446573 - Dev Loss: 0.465566\n",
      "Epoch 3 Iteration 920 - Train Loss: 0.447333 - Dev Loss: 0.464186\n",
      "Epoch 3 Iteration 930 - Train Loss: 0.403995 - Dev Loss: 0.462438\n",
      "Epoch 3 Iteration 940 - Train Loss: 0.447327 - Dev Loss: 0.461983\n",
      "Epoch 3 Iteration 950 - Train Loss: 0.427068 - Dev Loss: 0.462183\n",
      "Epoch 3 Iteration 960 - Train Loss: 0.422605 - Dev Loss: 0.461120\n",
      "Epoch 3 Iteration 970 - Train Loss: 0.423640 - Dev Loss: 0.455372\n",
      "Epoch 3 Iteration 980 - Train Loss: 0.426995 - Dev Loss: 0.453802\n",
      "Epoch 3 Iteration 990 - Train Loss: 0.401451 - Dev Loss: 0.450199\n",
      "Epoch 3 Iteration 1000 - Train Loss: 0.417996 - Dev Loss: 0.447924\n",
      "Epoch 3 Iteration 1010 - Train Loss: 0.408990 - Dev Loss: 0.445263\n",
      "Epoch 3 Iteration 1020 - Train Loss: 0.394200 - Dev Loss: 0.442505\n",
      "Epoch 4 Iteration 1030 - Train Loss: 0.379485 - Dev Loss: 0.438109\n",
      "Epoch 4 Iteration 1040 - Train Loss: 0.390961 - Dev Loss: 0.436028\n",
      "Epoch 4 Iteration 1050 - Train Loss: 0.363856 - Dev Loss: 0.433900\n",
      "Epoch 4 Iteration 1060 - Train Loss: 0.354601 - Dev Loss: 0.433439\n",
      "Epoch 4 Iteration 1070 - Train Loss: 0.395914 - Dev Loss: 0.430990\n",
      "Epoch 4 Iteration 1080 - Train Loss: 0.374439 - Dev Loss: 0.431542\n",
      "Epoch 4 Iteration 1090 - Train Loss: 0.352463 - Dev Loss: 0.427696\n",
      "Epoch 4 Iteration 1100 - Train Loss: 0.350640 - Dev Loss: 0.428141\n",
      "Epoch 4 Iteration 1110 - Train Loss: 0.356931 - Dev Loss: 0.427190\n",
      "Epoch 4 Iteration 1120 - Train Loss: 0.376602 - Dev Loss: 0.423336\n",
      "Epoch 4 Iteration 1130 - Train Loss: 0.359453 - Dev Loss: 0.421364\n",
      "Epoch 4 Iteration 1140 - Train Loss: 0.360815 - Dev Loss: 0.425069\n",
      "Epoch 4 Iteration 1150 - Train Loss: 0.334242 - Dev Loss: 0.417796\n",
      "Epoch 4 Iteration 1160 - Train Loss: 0.355165 - Dev Loss: 0.416998\n",
      "Epoch 4 Iteration 1170 - Train Loss: 0.369729 - Dev Loss: 0.419518\n",
      "Epoch 4 Iteration 1180 - Train Loss: 0.345730 - Dev Loss: 0.413230\n",
      "Epoch 4 Iteration 1190 - Train Loss: 0.312115 - Dev Loss: 0.414266\n",
      "Epoch 4 Iteration 1200 - Train Loss: 0.334117 - Dev Loss: 0.411711\n",
      "Epoch 4 Iteration 1210 - Train Loss: 0.317702 - Dev Loss: 0.416778\n",
      "Epoch 4 Iteration 1220 - Train Loss: 0.326430 - Dev Loss: 0.408102\n",
      "Epoch 4 Iteration 1230 - Train Loss: 0.350986 - Dev Loss: 0.409565\n",
      "Epoch 4 Iteration 1240 - Train Loss: 0.336631 - Dev Loss: 0.410908\n",
      "Epoch 4 Iteration 1250 - Train Loss: 0.327892 - Dev Loss: 0.407358\n",
      "Epoch 4 Iteration 1260 - Train Loss: 0.336803 - Dev Loss: 0.408122\n",
      "Epoch 4 Iteration 1270 - Train Loss: 0.363484 - Dev Loss: 0.402846\n",
      "Epoch 5 Iteration 1280 - Train Loss: 0.333022 - Dev Loss: 0.399123\n",
      "Epoch 5 Iteration 1290 - Train Loss: 0.328717 - Dev Loss: 0.396576\n",
      "Epoch 5 Iteration 1300 - Train Loss: 0.316681 - Dev Loss: 0.395785\n",
      "Epoch 5 Iteration 1310 - Train Loss: 0.316831 - Dev Loss: 0.398131\n",
      "Epoch 5 Iteration 1320 - Train Loss: 0.271478 - Dev Loss: 0.391975\n",
      "Epoch 5 Iteration 1330 - Train Loss: 0.266018 - Dev Loss: 0.392384\n",
      "Epoch 5 Iteration 1340 - Train Loss: 0.286464 - Dev Loss: 0.396755\n",
      "Epoch 5 Iteration 1350 - Train Loss: 0.320366 - Dev Loss: 0.393867\n",
      "Epoch 5 Iteration 1360 - Train Loss: 0.296127 - Dev Loss: 0.390053\n",
      "Epoch 5 Iteration 1370 - Train Loss: 0.299244 - Dev Loss: 0.387496\n",
      "Epoch 5 Iteration 1380 - Train Loss: 0.314226 - Dev Loss: 0.386092\n",
      "Epoch 5 Iteration 1390 - Train Loss: 0.275632 - Dev Loss: 0.386271\n",
      "Epoch 5 Iteration 1400 - Train Loss: 0.281493 - Dev Loss: 0.384720\n",
      "Epoch 5 Iteration 1410 - Train Loss: 0.287892 - Dev Loss: 0.382684\n",
      "Epoch 5 Iteration 1420 - Train Loss: 0.260930 - Dev Loss: 0.385977\n",
      "Epoch 5 Iteration 1430 - Train Loss: 0.286882 - Dev Loss: 0.383142\n",
      "Epoch 5 Iteration 1440 - Train Loss: 0.267674 - Dev Loss: 0.381457\n",
      "Epoch 5 Iteration 1450 - Train Loss: 0.285873 - Dev Loss: 0.381429\n",
      "Epoch 5 Iteration 1460 - Train Loss: 0.287926 - Dev Loss: 0.380514\n",
      "Epoch 5 Iteration 1470 - Train Loss: 0.283762 - Dev Loss: 0.380476\n",
      "Epoch 5 Iteration 1480 - Train Loss: 0.267323 - Dev Loss: 0.375613\n",
      "Epoch 5 Iteration 1490 - Train Loss: 0.270804 - Dev Loss: 0.376053\n",
      "Epoch 5 Iteration 1500 - Train Loss: 0.278417 - Dev Loss: 0.373398\n",
      "Epoch 5 Iteration 1510 - Train Loss: 0.285909 - Dev Loss: 0.373709\n",
      "Epoch 5 Iteration 1520 - Train Loss: 0.281221 - Dev Loss: 0.374650\n",
      "Epoch 5 Iteration 1530 - Train Loss: 0.254098 - Dev Loss: 0.371757\n"
     ]
    }
   ],
   "source": [
    "# Load datasets.\n",
    "train_dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu', 4096)\n",
    "dev_dataset = CoNLLDataset('./datasets/pt_bosque-ud-dev.conllu', 1024)\n",
    "\n",
    "dev_dataset.token_vocab = train_dataset.token_vocab\n",
    "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(train_dataset.token_vocab)\n",
    "output_vocab_size = len(train_dataset.pos_vocab)\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "n_layers = 1\n",
    "\n",
    "# Initialize the model.\n",
    "model = Tagger(input_vocab_size, output_vocab_size, n_layers)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss function weights.\n",
    "weight = torch.ones(output_vocab_size)\n",
    "weight[0] = 0\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "    \n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss(weight)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #losses.append(loss.data[0])\n",
    "        losses.append(loss.item())\n",
    "        if (i % 10) == 0:\n",
    "            # Compute dev loss over entire dev set.\n",
    "            # NOTE: This is expensive. You may want to only use a \n",
    "            # subset of the dev set.\n",
    "            #print('iteration, ', i)\n",
    "            dev_losses = []\n",
    "            for inputs, targets, lengths in dev_loader:\n",
    "                outputs, _ = model(inputs, lengths=lengths)\n",
    "                outputs = outputs.view(-1, output_vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                dev_losses.append(loss.item())\n",
    "            avg_train_loss = np.mean(losses)\n",
    "            avg_dev_loss = np.mean(dev_losses)\n",
    "            losses = []\n",
    "            #print('here')\n",
    "            print('Epoch %i Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (epoch, i, avg_train_loss, avg_dev_loss), end='\\n')\n",
    "            torch.save(model, 'pos_tagger_gru.pt')\n",
    "        i += 1\n",
    "        \n",
    "torch.save(model, 'pos_tagger_gru.final.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a09b623b-c4e3-407c-bc08-6052790330af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy - 0.876033\n",
      "\n",
      "F1-scores:\n",
      "\n",
      "NOUN - 0.825134\n",
      "DET - 0.961605\n",
      "ADP - 0.960512\n",
      "PUNCT - 0.998469\n",
      "VERB - 0.087612\n",
      "PROPN - 0.695402\n",
      "_ - 0.978022\n",
      "ADJ - 0.647141\n",
      "ADV - 0.864245\n",
      "PRON - 0.883005\n",
      "CCONJ - 0.987085\n",
      "AUX - 0.889673\n",
      "SCONJ - 0.677596\n",
      "NUM - 0.753521\n",
      "SYM - 1.000000\n",
      "X - 0.000000\n",
      "INTJ - 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Collect the predictions and targets\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets, lengths in dev_loader:\n",
    "    outputs, _ = model(inputs, lengths=lengths)\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    targets = targets.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    if torch.cuda.is_available():\n",
    "        targets = targets.cpu()\n",
    "        preds = preds.cpu()\n",
    "    y_true.append(targets.data.numpy())\n",
    "    y_pred.append(preds.data.numpy())\n",
    "\n",
    "# Stack into numpy arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
    "print('Accuracy - %0.6f\\n' % acc)\n",
    "\n",
    "# Evaluate f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(y_true, y_pred, average=None)\n",
    "print('F1-scores:\\n')\n",
    "for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
    "    print('%s - %0.6f' % (label, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9a8947d2-0903-485f-b525-4e05c75a8263",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('pos_tagger_gru.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    for word, tag in zip(sentence, pos_tags):\n",
    "        print('%s - %s' % (word, tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e296101-701e-44eb-8937-ca35eaf8570d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_labels(sentence, labels):\n",
    "    #print(sentence)\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    print(ids)\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    #labels = [dataset.pos_vocab.id2word(x) for x in labels]\n",
    "    #sentence = [test_dataset.token_vocab.id2word(x) for x in ids]\n",
    "    for word, tag, label in zip(sentence, pos_tags, labels):\n",
    "        print('%s - %s - %s' % (word, tag, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1606b1e-fe40-4cbc-a83d-feaf4c929e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Os', 'policiais', 'federais', 'de', 'Mato', 'Grosso', 'do', 'de', 'o', 'Sul', 'entraram', 'em', 'greve', 'ontem', ',', 'em', 'adesão', 'ao', 'a', 'o', 'movimento', 'iniciado', 'no', 'em', 'o', 'Distrito', 'Federal', '.']\n",
      "[[48], [1361], [3935], [2], [3265], [3266], [11], [2], [5], [496], [3737], [7], [1219], [58], [4], [7], [3524], [31], [3], [5], [962], [6250], [22], [7], [5], [15291], [653], [6]]\n",
      "Os - SCONJ - DET\n",
      "policiais - CCONJ - NOUN\n",
      "federais - VERB - ADJ\n",
      "de - ADP - ADP\n",
      "Mato - VERB - PROPN\n",
      "Grosso - ADJ - PROPN\n",
      "do - PRON - _\n",
      "de - ADP - ADP\n",
      "o - DET - DET\n",
      "Sul - NOUN - PROPN\n",
      "entraram - VERB - VERB\n",
      "em - ADP - ADP\n",
      "greve - NOUN - NOUN\n",
      "ontem - SYM - ADV\n",
      ", - PRON - PUNCT\n",
      "em - ADP - ADP\n",
      "adesão - NOUN - NOUN\n",
      "ao - ADJ - _\n",
      "a - PUNCT - ADP\n",
      "o - DET - DET\n",
      "movimento - PROPN - NOUN\n",
      "iniciado - VERB - VERB\n",
      "no - _ - _\n",
      "em - ADP - ADP\n",
      "o - DET - DET\n",
      "Distrito - PROPN - PROPN\n",
      "Federal - NOUN - PROPN\n",
      ". - PUNCT - PUNCT\n"
     ]
    }
   ],
   "source": [
    "test_dataset = CoNLLDataset('./datasets/pt_bosque-ud-test.conllu')\n",
    "dataset = CoNLLDataset('./datasets/pt_bosque-ud-train.conllu')\n",
    "\n",
    "sentence, labels = test_dataset[10]\n",
    "sentence = [test_dataset.token_vocab.id2word(x) for x in sentence]\n",
    "print(sentence)\n",
    "labels = [test_dataset.pos_vocab.id2word(x) for x in labels]\n",
    "inference_with_labels(sentence, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
